---
title: Softmax Regression
author: andrea perlato
date: '2019-11-26'
slug: softmax-regression
categories:
  - artificial intelligence
tags:
  - softmax
---



<style>
body {
text-align: justify}
</style>
<p>When we have to deal with a classification with more than 2 possible levels, we use a generalization of the logistic regression function called <a href="https://en.wikipedia.org/wiki/Softmax_function"><strong>softmax regression</strong></a>; a logistic regression class for multi-class classification tasks. In Softmax Regression, we replace the sigmoid logistic function by the so-called softmax function.</p>
<p><span class="math display">\[
\begin{array}{l}{\qquad P\left(y=j | z^{(i)}\right)=\phi_{\text {softmax}}\left(z^{(i)}\right)=\frac{e^{z^{(i)}}}{\sum_{j=0}^{k} e^{z_{k}^{(i)}}}} \\ {\text { where we define the net input } z \text { as }} \\ {\qquad z=w_{1} x_{1}+\ldots+w_{m} x_{m}+b=\sum_{l=1}^{m} w_{l} x_{l}+b=\mathbf{w}^{T} \mathbf{x}+b}\end{array}
\]</span></p>
<p>The <strong>w</strong> is the weight vector, <strong>x</strong> is the feature vector of 1 training sample, and <strong>b</strong> is the bias unit. </br>
A <strong>bias unit</strong> is an <strong>extra neuron</strong> added to each pre-output layer that stores the value of 1. Bias units aren’t connected to any previous layer and in this sense don’t represent a true activity. It is used in the case the sum of the weights is equal to zero. </br>
Now, this softmax function computes the probability that this training sample x(i) belongs to class j given the weight and net input z(i). So, we compute the probability <strong>p(y=j∣x(i);wj</strong>) for each class label in j=1,…,k.. Note the normalization term in the denominator which causes these class probabilities to sum up to one.</p>
<p>Even if it is a bit unusual, we use the softmax regression as the activation function of the <strong>output layer y</strong>.</p>
<p><span class="math display">\[
\begin{array}{l}{t=e^{\left(z^{(1)}\right)}} \\ {a^{(L)}=\frac{e^{z^{(L)}}}{\sum_{j={1}}^{4} t_{i}}, \quad a_{i}^{(L)}=\frac{t_{i}}{\sum_{j={1}}^{\frac{4} t_{i}}}}\end{array}
\]</span>
The formula above assumes that we have 4 layers L on the output y as depicted in the figure below.</p>
<center>
<img src="/img/softmaxregression.png" style="width:80.0%" />
</center>
<p>In the figure above we have z^(L) set to (5, 2, -1, 3), and from these values we can compute the activation function t suing the formula represented in the calculation we made before. Having the value of <strong>t</strong>, we can calculate the activation function <strong>a</strong> for the output <strong>y</strong>.</p>
