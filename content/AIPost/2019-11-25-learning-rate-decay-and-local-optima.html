---
title: Learning Rate Decay and Local Optima
author: andrea perlato
date: '2019-11-25'
slug: learning-rate-decay-and-local-optima
categories:
  - artificial intelligence
tags:
  - local minimum
  - learning rate decay
---



<style>
body {
text-align: justify}
</style>
<p>Supposing we are implementing a mini-batch gradient descent of just 64 or 128 examples. During the interation we can occur to the problem to not converge to the minimum. That is expecially true when we use fixed values of alpha learning rate. On the contrary, when we slowly reduce the learning rate alpha we are able to end up oscillating in the region around the minimum.</p>
<p>First, we have to remember that <strong>one epoch</strong> is one pass through the training set.</p>
<p><span class="math display">\[
\alpha=\frac{1}{1+d_{\text {decay-rate}} * \text { epoch-num }} \alpha_{0}
\]</span>
Where <strong>decay-rate</strong> is an hyperparameter that we need to tune, and <strong>apha0</strong> is the initial learning rate.
If we start with a alha0=0.2 and a decay-rate=1 and using the formula above, then the alpha tends to decrease over the epochs. So, to use learning rate decay we can try a variety of both hyperparameters alpha0 and decay-rate.
We can also use other method: </br>
1 - the so called <strong>exponential decay</strong> where alpha is some numbers less than 1. </br>
The calculation is: <strong>aipha=0.95^epoch-num * alpha0</strong> </br>
2 - there is also the <strong>discrete stair-case</strong>. The calculation is: <strong>alpha=k/sqrt(epoch-num) * alpha0</strong> </br>
3 - some people also use the <strong>manual decay</strong>. Here we just check the time for training our model and we stop if the process is too long. At this point, we manually descease the alpha hyperparameter to speed up the process. Of course, this is feasible only if we are dealing with a small training-set.</p>
<p><strong>The problem of local optima</strong> </br>
Looking at the figure here below on the left, it is quite common to get stuck to a local minimum rather than find the way to a global optimum.
In reality, most of the points of zero gradient are not local optima like in the figure on the left, but instead most points of zero gradient are in a so called <strong>saddle points</strong> as pictured on the right figure.</p>
<center>
<img src="/img/plateau.png" style="width:80.0%" />
</center>
<p>The <strong>saddle point</strong> has derivatives equal to zero and we have the <strong>Problem of Plateaus</strong> which is a region where the derivative is close to zero foor a long time.
The <strong>Plateau</strong> can really slow down the learnind. The plateau is represented by the red arrow on the right figure. In fact, the process takes long time before realize to follow the green path in order to reach the minimum. </br>
This is the scenario where more sophisticated observation algorithms, such as AdaM, can actually speed up the learning-rate.</p>
<p>Reference: <a href="https://www.coursera.org/learn/deep-neural-network/home/welcome"><strong>coursera deep neural network course</strong></a></p>
