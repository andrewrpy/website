---
title: Learning Rate Decay and Local Optima
author: andrea perlato
date: '2019-11-25'
slug: learning-rate-decay-and-local-optima
categories:
  - artificial intelligence
tags:
  - local minimum
  - learning rate decay
---

<style>
body {
text-align: justify}
</style>

Supposing we are implementing a mini-batch gradient descent of just 64 or 128 examples. During the interation we can occur to the problem to not converge to the minimum. That is expecially true when we use fixed values of alpha learning rate. On the contrary, when we slowly reduce the learning rate alpha we are able to end up oscillating in the region around the minimum. 

First, we have to remember that **one epoch** is one pass through the training set.

$$
\alpha=\frac{1}{1+d_{\text {decay-rate}} * \text { epoch-num }} \alpha_{0}
$$
Where **decay-rate** is an hyperparameter that we need to tune, and **apha0** is the initial learning rate.
If we start with a alha0=0.2 and a decay-rate=1 and using the formula above, then the alpha tends to decrease over the epochs. So, to use learning rate decay we can try a variety of both hyperparameters alpha0 and decay-rate.
We can also use other method: </br>
1 - the so called **exponential decay** where alpha is some numbers less than 1. </br> 
The calculation is: **aipha=0.95^epoch-num * alpha0** </br>
2 - there is also the **discrete stair-case**. The calculation is: **alpha=k/sqrt(epoch-num) * alpha0** </br>
3 - some people also use the **manual decay**. Here we just check the time for training our model and we stop if the process is too long. At this point, we manually descease the alpha hyperparameter to speed up the process. Of course, this is feasible only if we are dealing with a small training-set.




































