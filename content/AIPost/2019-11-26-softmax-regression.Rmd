---
title: Softmax Regression
author: andrea perlato
date: '2019-11-26'
slug: softmax-regression
categories:
  - artificial intelligence
tags:
  - softmax
---

<style>
body {
text-align: justify}
</style>

When we have to deal with a classification with more than 2 possible levels, we use a generalization of the logistic regression function called [**softmax regression**](https://en.wikipedia.org/wiki/Softmax_function); a logistic regression class for multi-class classification tasks. In Softmax Regression, we replace the sigmoid logistic function by the so-called softmax function.

$$
\begin{array}{l}{\qquad P\left(y=j | z^{(i)}\right)=\phi_{\text {softmax}}\left(z^{(i)}\right)=\frac{e^{z^{(i)}}}{\sum_{j=0}^{k} e^{z_{k}^{(i)}}}} \\ {\text { where we define the net input } z \text { as }} \\ {\qquad z=w_{1} x_{1}+\ldots+w_{m} x_{m}+b=\sum_{l=1}^{m} w_{l} x_{l}+b=\mathbf{w}^{T} \mathbf{x}+b}\end{array}
$$

the **w** is the weight vector, **x** is the feature vector of 1 training sample, and **b** is the bias unit. </br>
A **bias unit** is an **extra neuron** added to each pre-output layer that stores the value of 1. Bias units aren't connected to any previous layer and in this sense don't represent a true activity. It is used in the case the sum of the weights is equal to zero.
Now, this softmax function computes the probability that this training sample x(i) belongs to class j given the weight and net input z(i). So, we compute the probability **p(y=j∣x(i);wj**) for each class label in j=1,…,k.. Note the normalization term in the denominator which causes these class probabilities to sum up to one.



