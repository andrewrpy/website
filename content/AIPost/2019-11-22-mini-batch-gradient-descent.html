---
title: Mini-batch Gradient Descent
author: andrea perlato
date: '2019-11-22'
slug: mini-batch-gradient-descent
categories:
  - artificial intelligence
tags:
  - gradient descent
---



<style>
body {
text-align: justify}
</style>
<p>In <strong>Batch Gradient Descent</strong> on every interation we go through the entire training set.
From the figure below we can see the <strong>cost function J</strong> on the left a batch gradient descent that decrease every single interation. On the right we have the <strong>cost function J</strong> of a mini-batch gradient descent where in every interatin our processing in training on a different train-set; that is why the loss function J is going to be a little noisier.</p>
<center>
<img src="/img/minibatchgraddesc.png" style="width:60.0%" />
</center>
<p>The most important parameter we have to choose is the <strong>size of the mini-batch</strong> called <strong>m</strong>.
What works best is to use something <strong>in-between Mini-batch size</strong> not to big or too small in order to have fatser learing. Moreover, we have more vectorization and we make progress without needing to wait to process the entire training-set. </br>
The <strong>in-between Mini-batch size</strong> doesnâ€™t always garantee head toward the minimim, but it tends to head more consistently on the minimum. </br>
Some guidelines in choosing the size of the in-between Mini-batch size are: </br>
1 - with small training-set: use batch gradient descent </br>
2 - with big training.set: mini-batch size is typical in the range of the power of 2 (64, 128, 256, 512) </br>
3 - make sure that mini-batch fits in CPU/GPU memory. </br></p>
<p>The size of <strong>in-between Mini-batch size</strong> is an <strong>hyperparameter</strong> and it is common to try few different powers of 2 between the range that goes from 64 to 512 and then pick one that makes out gradient descent as efficient as possible.</p>
