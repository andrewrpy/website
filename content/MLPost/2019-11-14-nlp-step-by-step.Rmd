---
title: NLP Step by Step
author: andrea perlato
date: '2019-11-14'
slug: nlp-step-by-step
categories:
  - text mining
tags:
  - NLP
  - tf-idf
  - naive bayes
---

<style>
body {
text-align: justify}
</style>

This post has the aim to shows all the processes related to the NLP and how to use the Naive Bayes Classifier using Python and the **nltk** library.
We use data from spam detection.

```{r setup, include=FALSE}
# Sys.which("python") # check py path
library(reticulate)
use_python("C:/Users/Utente/Anaconda3/python.exe", required=TRUE)

```

```{python, include=FALSE}
import pandas as pd
import numpy as np
import nltk
#import matplotlib.pyplot as plt
#import seaborn as sns

messages = pd.read_csv('C:/07 - R Website/dataset/PY/SMSSpamCollection', sep='\t', names=['label','message'])
# messages.head()

```

In NLP a large part of the processing is Feature Engeneering. Tipically the steps are: </br>
**Regular Expression**: that is a formal language for specifying text strings: for example, we can have for the same word the s for plural, the capital first letter and any combination of those. We need a methodology to deal with this. </br>
**Word Tokenization**: every language processing has to minimize the text in some way, and we have to start with segmentation also known as tokenization of the words. 
We have to find the **lemma** that is same **stem** or part of the word with the same root. Ones we tokenized our words we need to normalize that words, and also, we have to stem that word in order to reduce inflected words. Moreover, we have to reduce the uppercase in lowercase, with only some exceptions like: General Motors or FED for Federal Reserve Bank, that could be especially helpful in machine translation. </br>
**Minimum Edit Distance**: is the way of solving string similarities. It is ery helpful to correct grammar errors. So, the Minimum Edit Distance between two strings is the minimum number of editing operations (insertion, deletion, substitution) that are needed to transform one string into the other. We often need to use the so called **Backtrace Method**: similar to [**DTW**](https://en.wikipedia.org/wiki/Dynamic_time_warping) to find the minimum distance between two words. </br>
**Object Standardization**: in a text, are often present words or phrases which are not present in a standard lexiconsuch as acronyms, hashtags, colloquial slang. These are very present on social media. We can decide to include them in the stop words list. </br>
**Bag of words**: is used to extract features. We simply count the number of occurrences for each word, this process also called **CountVectorizer**. To make the CountVectorizer more comparable, we scale it using the Term Frequency Transformation **TF**, and in order to boost the most important features we use the Inverse Document Frequency **IDF**, this calculate how often a word occurs in the corpus. The combination of both is called **TFiDF** = TF * IDF.  </br>

$$
\begin{aligned} t f \cdot d t(t, D) &=t f(t, d) \cdot d f(t, D) \\ t f(t, d) &=f_{t \mu} \\ i d f(t, D) &=\log \left(\frac{N}{|\{d \in D: t \in A\}|}\right) \end{aligned}
$$
**TFiDF** is the LOG of the total number of documents N divided by the number of documents that contain the term that we are taking into consideration.
From the expression above, we have **D** the toal number of documents, and **t** number of documents with the term.
Matematically we can use the expression reported here below.

<center>
![](/img/tfidfnlp.png){width=40%}

</center>

The expression above determ the word **Wx** in the document **y**. The formula above give us not only the conut, but also the notation of the importance of a word in the entire corpus of all documents. </br>
Now we start with some explortatory data analysis and feature engineering using Python.

```{python, include=TRUE}
messages.groupby('label').describe() # most popular ham and spam messages

messages['length'] = messages['message'].apply(len) # length of the messages
messages.head()

messages['length'].describe()

```

From the code above we can see we have a etxt with 910 characters. Here below the text of the message.

```{python, include=TRUE}
messages[messages['length'] == 910]['message'].iloc[0]

```

There are many methods to convert a corpus of strings to a vector format and the simplest way is usiing **bag of words** where each unique word in the text will represented by one vector. The first step is to remove punctuation, and stopwords.

```{python, include=TRUE}
import string
from nltk.corpus import stopwords

def text_process(mess):
    """
    Takes in a string of text, then performs the following:
    1. Remove all punctuation
    2. Remove all stopwords
    3. Returns a list of the cleaned text
    """
    # Check characters to see if they are in punctuation
    nopunc = [char for char in mess if char not in string.punctuation]

    # Join the characters again to form the string.
    nopunc = ''.join(nopunc)
    
    # Now just remove any stopwords
    return [word for word in nopunc.split() if word.lower() not in stopwords.words('english')]
    
# Check it
messages['message'].head(5).apply(text_process)
    
```

Now, we can focus on vectorization namely count how many times a word occur in a text, and this is called **Term Frequency**. We have also to **normalize** the vector to unit length. Since there are so many messages, we can expect lot of zeros. Because of this, we can use **SciKit Learn** to otput a [**Sparse Matrix**](https://en.wikipedia.org/wiki/Sparse_matrix).

```{python, include=TRUE}
from sklearn.feature_extraction.text import CountVectorizer

# Create bag of words
bow_transformer = CountVectorizer(analyzer=text_process).fit(messages['message'])

# Check the sparsity
messages_bow = bow_transformer.transform(messages['message'])
print('Shape of Sparse Matrix: ', messages_bow.shape)
print('Amount of Non-Zero occurences: ', messages_bow.nnz)

sparsity = (100.0 * messages_bow.nnz / (messages_bow.shape[0] * messages_bow.shape[1]))
print('sparsity: {}'.format(sparsity))

```

Now that we have an idea about the sparsity of of bag of word, we can 































