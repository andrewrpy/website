---
title: Exploratory Data Analysis
author: andrea perlato
date: '2019-12-17'
slug: exploratory-data-analysis
categories:
  - machine learning
tags:
  - EDA
  - Exploratory Data Analysis
---

<style>
body {
text-align: justify}
</style>

Before the definition of the model it is always important to plot the distribution of our variable, finding unusual observation (points of leverage, hat value and outliers), missing data (understand if they are missing at random or not at random). Then, I always put the data in a multiple regression to see correlations and the presence of [**multicollinearity**](https://en.wikipedia.org/wiki/Multicollinearity): is the phenomenon in which one predictor can be linearly predict from others with a substantial degree of accuracy. In this situation, the coefficients estimated may change erratically in response to small changes of the model. For this reason, I always test it using the [**Variance Inflation Factor Analysis VIF**](https://en.wikipedia.org/wiki/Variance_inflation_factor) that test the ratio of variance in the model with multiple terms, dividing by the variance of the model with one term alone: so, it tests the severity of multicollinearity.
Then, we can reduce the predictors finding the best ones using the [**Stepwise**](https://en.wikipedia.org/wiki/Stepwise_regression) approach (comparing AIC and BIC index) and then the [**Relative Importance Weight**](https://www.listendata.com/2015/05/relative-importance-weight-analysis.html) Analysis: in multiple regression analysis, we are interested in determining the relative contribution of each predictor. The  Relative Importance Weight Analysis, is a method by which we can partition the R-squared into pseudo-orthogonal portions. Each portion representing the relative contribution of one predictor variable.
























