---
title: Exploratory Data Analysis
author: andrea perlato
date: '2019-12-17'
slug: exploratory-data-analysis
categories:
  - machine learning
tags:
  - EDA
  - Exploratory Data Analysis
---

<style>
body {
text-align: justify}
</style>

Before the definition of the model it is always important to plot the distribution of our variable, finding unusual observation (points of leverage, hat value and outliers), missing data (understand if they are missing at random or not at random). Then, I always put the data in a multiple regression to see correlations and the presence of [**multicollinearity**](https://en.wikipedia.org/wiki/Multicollinearity): is the phenomenon in which one predictor can be linearly predict from others with a substantial degree of accuracy. In this situation, the coefficients estimated may change erratically in response to small changes of the model. For this reason, I always test it using the [**Variance Inflation Factor Analysis VIF**](https://en.wikipedia.org/wiki/Variance_inflation_factor) that test the ratio of variance in the model with multiple terms, dividing by the variance of the model with one term alone: so, it tests the severity of multicollinearity.
Then, we can reduce the predictors finding the best ones using the [**Stepwise**](https://en.wikipedia.org/wiki/Stepwise_regression) approach (comparing AIC and BIC index) and then the [**Relative Importance Weight**](https://www.listendata.com/2015/05/relative-importance-weight-analysis.html) Analysis: in multiple regression analysis, we are interested in determining the relative contribution of each predictor. The  Relative Importance Weight Analysis, is a method by which we can partition the R-squared into pseudo-orthogonal portions. Each portion representing the relative contribution of one predictor variable.

**Scale** </br>
If we have to measure distances in KNN it is necessary to Scale the data. We can find opposite solution if we merely calculate distances without scaling. For example, we can use **MixMax Scaler**.

$$
X=(X-X \cdot \min ()) /(X \cdot \max ()-X \cdot \min ())
$$
Another method is using the **Z points**, with mean=0 and standard deviation=1.

$$
X=(X-X . \text { mean }()) / X . s t d()
$$

**Outliers** </br>
Linear models are influenced by Outliers. An observation is an outlier if it falls more than 1.5 IQR above the upper quantile or if it falls more than 1.5 IQR below the lower quantile. IQR is the Interquartile Range, is a measure of statistical dispersion, and is the difference between 75th and 25th percentiles: Q3-Q1.

**Log-Transformation** </br>
This move outliers closer to other values. For example, the log transformation drive outliers closer to the featuresâ€™ average.

**Feature Generation** </br>
It is a process of creating. We can use One-Hot Encoding OHE to reshape the data in order to have 1 when the feature is present and 0 when is not present.

<center>
![](/img/onehotencoding.png){width=70%}
</center>

Crucially, One-Hot Encoding is by default already scaled. We can use the Sparse Matrix in order to exclude the 0s and work only when the feature is present, by doing that we save space in RAM. Note that we can use sparse matrix only if features with 0 are far less of all the values.

























