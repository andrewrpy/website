---
title: The Learning Rate
author: andrea perlato
date: '2019-12-17'
slug: the-learning-rate
categories:
  - artificial intelligence
tags:
  - learinig rate
---

<style>
body {
text-align: justify}
</style>

Deep learning neural networks are trained using the [**stochastic gradient descent algorithm**](https://www.andreaperlato.com/aipost/stochastic-gradient-descent/).
Stochastic gradient descent is an optimization algorithm that estimates the error gradient for the current state of the model using examples from the training dataset, then updates the weights of the model using the backpropagation of errors algorithm.

The amount that the weights are updated during training is referred to as the **step size or the learning rate**.
Specifically, the learning rate is a configurable hyperparameter used in the training of neural networks that has a small positive value, often in the range between **0.0 and 1.0**.
During training, the **backpropagation** of error estimates the amount of error for which the weights of a node in the network are responsible. Instead of updating the weight with the full amount, it is scaled by the learning rate. This means that a learning rate of 0.1, a traditionally common default value, would mean that weights in the network are updated 0.1 * (estimated weight error) or 10% of the estimated weight error each time the weights are updated.




































References: </br>
[**Deep Learing Adaptive Computation and Machine Learning Series**](https://www.amazon.com/Deep-Learning-Adaptive-Computation-Machine/dp/0262035618/ref=as_li_ss_tl?_encoding=UTF8&qid=&sr=&linkCode=sl1&tag=inspiredalgor-20&linkId=e4e32749958369afb667e7e4323d65ba&language=en_US) </br>
[**Machine Learning Mastery**](https://machinelearningmastery.com/learning-rate-for-deep-learning-neural-networks/)














