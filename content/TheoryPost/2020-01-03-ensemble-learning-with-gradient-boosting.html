---
title: Ensemble Learning with Gradient Boosting
author: andrea perlato
date: '2020-01-03'
slug: ensemble-learning-with-gradient-boosting
categories:
  - machine learning
tags:
  - gradient boosting
---



<style>
body {
text-align: justify}
</style>
<p>Ensemble often have lower error than any individual method by themselves.
It has also less overfitting leading us to a better performance in the test set.
Each kind of learners that we might use has a sort of bias. Combining all of them, can reduce this bias. </br></p>
<p><strong>Gradient Boost vs.Â AdaBoost</strong> </br>
Gradient Boost is similar with <a href="https://www.andreaperlato.com/theorypost/ensemble-learning/"><strong>AdaBoost</strong></a> that starts by building a very short tree called <strong>stump</strong> from the training data. The <strong>amount of say</strong> that the stump has on the final output is based on how well the stump compensated for those previous errors.</p>
