---
title: Ensemble Learning with Gradient Boosting
author: andrea perlato
date: '2020-01-03'
slug: ensemble-learning-with-gradient-boosting
categories:
  - machine learning
tags:
  - gradient boosting
---

<style>
body {
text-align: justify}
</style>

Ensemble often have lower error than any individual method by themselves. 
It has also less overfitting leading us to a better performance in the test set.
Each kind of learners that we might use has a sort of bias. Combining all of them, can reduce this bias. </br>

**Gradient Boost vs. AdaBoost** </br>
Gradient Boost is similar with [**AdaBoost**](https://www.andreaperlato.com/theorypost/ensemble-learning/) that starts by building a very short tree called **stump** from the training data. The **amount of say** that the stump has on the final output is based on how well the stump compensated for those previous errors.