---
title: Introduction to Topic Model
author: andrea perlato
date: '2019-08-08'
slug: introduction-to-topic-model
categories:
  - text mining
tags:
  - topic model
---



<style>
body {
text-align: justify}
</style>
<p>The <a href="https://en.wikipedia.org/wiki/Topic_model"><strong>Topic Model</strong></a> is a type of statistical model to find the topics that occur in a collection of documents.
It is a frequently used text-mining tool for discovery of hidden semantic structures in a text body. For example, image to have some articles or a series of social media messages and we want to understand what is going on inside of them. A common tool to face this problem is via <strong>Unsupervised Machine Learning model</strong>.
From a high level perspective, having a bunch of documents, and like in <a href="https://www.andreaperlato.com/theorypost/introduction-to-k-means-and-hierarchical-clustering/"><strong>K-Mean Clustering</strong></a> we want to find the <strong>K</strong> number of topics that best describe our corpus of text.</p>
<center>
<img src="/img/topics.png" style="width:40.0%" />
</center>
<p>From the figure above, we have three different topics: technology (yellow), business(orange), and arts(blue).
In addition to those topics, we also have an association to the documents to topics. In fact, a document can be entirely a technology topic (i.e. red light, green light, 2-tone led, simplify screen), but also a document that is a sort of mixture of two or more topic (see the grey text in the figure below).</p>
<center>
<img src="/img/mixturetopics.png" style="width:50.0%" />
</center>
<p>Topic Modelcan be seen as a <strong>Matrix Factorization Problem</strong>, where <strong>K</strong> is the number of topics, <strong>M</strong> is the number of documents, and <strong>V</strong> is the size of the vocabulary.</p>
<center>
<img src="/img/matrixfactorization.png" style="width:50.0%" />
</center>
<p>The <strong>MxV</strong> matrix corresponds to the distribution of the words for each of the topics,and to find this the <a href="https://en.wikipedia.org/wiki/Latent_semantic_analysis"><strong>Latent Semantic Analysis</strong></a> is widely used.</p>
<p>An alternative of the <strong>Matrix Factorization Problem</strong> is the <a href="https://en.wikipedia.org/wiki/Generative_model"><strong>Generative Model</strong></a>. </br>
More particularly, the <a href="https://en.wikipedia.org/wiki/Latent_Dirichlet_allocation"><strong>Latent Dirichlet Allocation</strong></a> is commonly used in topic modelling.</p>
<p><span class="math display">\[
P(\boldsymbol{p} | \alpha \boldsymbol{m})=\frac{\Gamma\left(\sum_{k} \alpha m_{k}\right)}{\prod_{k} \Gamma\left(\alpha m_{k}\right)} \prod_{k} p_{k}^{\alpha m_{k}-1}
\]</span></p>
<p>Here above, we have the Dirichlet Distribution Equation, where <strong>alpha</strong> is the variance, and <strong>m</strong> is the mean.</p>
<center>
<img src="/img/uniformlda.png" style="width:40.0%" />
</center>
<p>As described in the figure above, when we have a <strong>uniform mean</strong> (1/3), and an <strong>variance</strong> (alpha) of three, when we molltiply them togheter, the <strong>LDA=1</strong>, and so each topic is equally likely.
But when the <strong>variance</strong> is larger and larged the LDA is concentrating the distribution around the mean (the dark circle in the middle of the triangle). </br></p>
<p>There are other ways to parametrize the LDA distribution. </br>
For example, we can have mean in a differnt location (see the left and center figure below). </br>
We can also have the <strong>variance parameter alpha</strong> smaller than <strong>1</strong>. In this case we push the probability mass to the edges of the simplex (see the right figure below). In this case, with <strong>alpha&lt;1</strong> we have a preference for the multinomial distribution which is far avay for the center.
To be <strong>far away from the center</strong> means we have not a precise topic to assign to our word. It is similar to how people write document where many things are inside a concept.
In other words, the Dirichlet Distribution give us a distribution over all the places where the Multinomila Distribution can land.</p>
<center>
<img src="/img/nonuniformlda.png" style="width:40.0%" />
</center>
<p>The <strong>Dirichlet Distribution</strong> can be used to isolate which document is about a specific topic.
For each topic <strong>K</strong>, we have a multinomial distribution <strong>Betak</strong>, called <strong>Topic Distribution</strong> from a Dirichlet Distribution with parameter <strong>lambda</strong>.
The next step is to draw a <strong>multinomial distribution over topics</strong> represented by <strong>Өd</strong>. Once we have it, we can draw for each word the so called <strong>Topic Assignment</strong> represented in the figure below by <strong>Zn</strong>.
Till now, we don’t know what the word is. We have to look at the <strong>Topic Distribution Betak</strong> in order to generate the word that comes from the multinomial distribution.</p>
<center>
<img src="/img/ldaplatenotation.png" style="width:40.0%" />
</center>
<p>The graph above is a representation of the probabilistic model, also called <a href="https://en.wikipedia.org/wiki/Plate_notation"><strong>Plate Notation</strong></a>. </br>
As we can see we have in the Plate Notation <strong>K</strong> topics in <strong>M</strong> documents, and <strong>N</strong> words in each document. </br>
Crucially, the only thing that we observe are the <strong>words</strong>, and our task is to figure out what topic to assign <strong>Zn</strong>.</p>
<p>Ideally, once we have the collection of words per topic, is the topic is interpretable, people will consistently choose true <strong>Intruder</strong>, or define the words that didn’t belong.</p>
