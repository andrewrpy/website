---
title: Activation Function in Neural Nets
author: andrea perlato
date: '2020-01-09'
slug: activation-function-in-neural-nets
categories:
  - artificial intelligence
tags:
  - activation function
  - sigmoind
  - threshold
  - rectifier
  - tanh
---

<style>
body {
text-align: justify}
</style>

What an **artifical neuron** do is to calculate a **weighted sum** of its input, adds a bias and then decides whether it should be “fired” or not.
considering the following function:

$$
Y=\sum(\text {weight} * \text {input})+\text {bias}
$$

the value of Y can be anything ranging from -inf to +inf. The neuron really doesn’t know the bounds of the value. How do we decide whether the neuron should fire or not? We use **activatioin functions** for this purpose.

**Sigmoid Function** </br>
We can consider the **sigmoid function** that maps the values from 0 to 1 and mimics the biological neuron.
An important aspect to consider using the sigmoid function is that it makes the neural nework nonlinear, and we cannot reduce the NN into a simple linear equation.
In modern NN there are some problems with sigmoid and is not longer used as often except in some specific cases.

<center>
![](/img/sigmoidfunction.png){width=40%}

</center>

**Standardization** </br>
It is of high importance to standardize, because we don't want to have one rangein million and another range in millesimal; the **sigmoid** is problematic in this regard because it goes between 0 and 1 and the center is 0.5. As a conseguence, the output of a sigmoid can never be centerd around zero.
This recall the concept of **Uniformity** of NN. </br>
The solution for this is the **Hyperbolic tangent Function**.


**Hyperbolic tangent Function** </br>
It has the same shape of the sigmoid but centered around zero.

<center>
![](/img/hypebolicfunctio.png){width=40%}

</center>

This function is also called the **10 H** which is short for hyperbolic tangent. 
There are hyperbolic versions for all the trigonometric functions: sin, cos, tan. So, we are using the **tanh(x)=sinh(x)/cos(x)** that is the hyperbolic sine over the hyerbolic cosine, that results in the following equation:

$$
\tanh (a)=\frac{\exp (2 a)-1}{\exp (2 a)+1}
$$

which is similar to the sigmoid function. The major difference between the sigmoid and the 10 H is that the sigmoid goes between 0 and 1 while the 10 H goes between -1 and +1.

Although the 10 H is better than the sigmoid there are many other aspect to take in consideration. 
The major problem is the [**Vanishing Gradient**](https://www.andreaperlato.com/aipost/vanishing-gradient/). We have to find the gradient of the cost with respect to the parameters, but when we have very deep NN our gradient has to propagate backwards throughout the NN. The deeper a NN is, the more terms have to be multiplied in the gradient due to the **chain rule** of the calculus. So, multiple functions become multiplication in the derivative as shown below:

$$
\frac{\partial J}{\partial W^{(1)}}=\frac{\partial J}{\partial z^{(L)}} \frac{\partial z^{(L)}}{\partial z^{(L-1)}} \ldots \frac{\partial z^{(2)}}{\partial z^{(1)}} \frac{\partial z^{(1)}}{\partial W^{(1)}}
$$

The problem with **sigmoind** ere is that its derivativeis very nearly zero, and only the center is non-zero as shown in the figure below, and the maximum value is only **0.25**. By conseguence, when we moltiply a numbers that are very smallwe obtain a smaller one; the further we go back in the NN the smaller the gradient becomes. We call this the **Vanishing Gradient Problem**. 

<center>
![](/img/vangrad.png){width=90%}
</center>

Looking at the graph on the right of the figure above, we can see the magnitute of the gradient at each ayer of the NN as it is trained. We notice that the futher we go back, the smaller the gradient gets. The end result is that the weights close to the input are almost not trained.
The solution to train a very deep NN is to use the **Rectifier Linear Unit Function - ReU**.

**Rectifier Linear Unit Function - ReU** </br>
The ReLUfunctioin has a corner at zero where the derivative is technically not defined.

<center>
![](/img/rectifierfunction.png){width=40%}

</center>

Having values grater than zero never have a zero gradient and this makes the training of the NN more efficient.
The left half of the function has derivative that is exactly zero and so the function is already vanished. This is the so called phenomenon of the **Dead Neuron**.

**Fixing Dead Neuron** </br>
Some researchers have tried to make modification to solve the dead neuron phenomenon. One alternative is the **Leaky ReLU Function - LReLU**. The LReLU has small positive slope for negative inputs (like for example 0.1). Using LReLU our derivatives will always be positive

<center>
![](/img/LReLU.png){width=70%}
</center>

There are other optional function to solve the Dead Neuron such as the **Exponential Linear UnitFunction - ELU** which has a more steadly decreasing value on the left side of the fnction. This activation function speeds up learning and leads to higher accuracy than the LReLU.
One interesting aspect of the ELU is that it allows its output to be negative and this goes to the idea to have the mean of the value close to zeor.
Another option very similar to the ELU is the **Softplus Function**. </br>
As we can see from the formula below it takes the log of the exponent and for this reason the function look very linear where the inputs are very large as depicted in the figure below.

$$
f(x)=\log \left(1+e^{x}\right)
$$
<center>
![](/img/softplus.png){width=40%}
</center>

For both **Softplus** and **ELU** there is thevanishing gradient on the left side but it is not so musch of a problem because the function works.
Moreover they are in the range of of 0 and infinity and this means they are not centerd around zero.

Despite all these alternatives for the ReLU function, most people still use the ReLU as a reasonable defautl choice. The suggestion is that NN is experimentation and we have to try also to use Softplus and ELU.






































