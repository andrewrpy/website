---
title: Introduction to Support Vector Machine
author: andrea perlato
date: '2019-10-02'
slug: introduction-to-support-vector-machine
categories:
  - machine learning
tags:
  - suppor vector machine
---

<style>
body {
text-align: justify}
</style>

We use as an example the measurement of the **Mass of mice (g)**. The red dots in the figure below represent mice that are not obese and the green dots represent mice obese. Based on this observation, we can pick a threshold, and when we have a new observation that has less mass than the threshold we can classify it as not obese. And when we have a new observation with more mass than the threshold we can classify it as obese. However, when we have a new observation just above the threshold we classify it as obese, but it doesnâ€™t make a lot of sense, because the observation is much closer to the not obese observations, as depicted in the figure here below. So, this threshold is pretty lame.

<center>
![](/img/svm1.png){width=60%}
</center>

To solve this problem, we can focus on the observations on the edges of each cluster and use the midpoint between them as the threshold called **Maximal Margin Classifier**. Now, with this method, the same observation we considered before is closer to the not obese mice than iti is to the obese mice. So, it makes sense to classify this observation as not obese.

<center>
![](/img/svm2.png){width=60%}
</center>

But actually, the **Maximal Margin Classifier** is not the best solution to classify our mice. If fact we could have an **outlier** observation that was classified as not obese (i.e. big mouse not obese), but was much closer to the obese observation. In this case the Maximal Margin Classifier would be super close to the obese observation and relly far from the majority ot the not obese mice. So, Maximal Margin Classifier is super sensitive to **outliers**. See figure below.

<center>
![](/img/svm3.png){width=60%}

</center>

To make a threshold that is not so sensitive to the outliers **we must allow misclassification**. Choosing a threshold that allows misclassifications (we not take in consideration outliers) is an example of **Bias/Variance Tradeoff** that plagues all of machine learning.
When we allow misclassification, the distance between the observations and the threshold is called a **Soft Margin**.  To find the best Soft Margin we use **Cross Validation** to determine how many misclassifications (outliers) and observations to allow inside the Soft Margin to get the best classification.
When we use a Soft Margin to determine the location of a threshold, then we are using a **Soft Margin Classifier** aka a **Support Vector Classifier** to classify observations.














































