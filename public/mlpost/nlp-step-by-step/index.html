<!DOCTYPE html>
<html lang="en-us">
  <head>
    <meta charset="utf-8">
<meta name="viewport" content="width=device-width, initial-scale=1.0">
<meta name="generator" content="Hugo 0.55.6" />


<title>NLP Step by Step - Andrea Perlato</title>
<meta property="og:title" content="NLP Step by Step - Andrea Perlato">



  








<link href='//cdnjs.cloudflare.com/ajax/libs/highlight.js/9.12.0/styles/atom-one-dark.min.css' rel='stylesheet' type='text/css' />



<link rel="stylesheet" href="/css/fonts.css" media="all">
<link rel="stylesheet" href="/css/main.css" media="all">



  </head>
  <body>
    <div class="wrapper">
      <header class="header">
        <nav class="nav">
  <a href="/" class="nav-logo">
    <img src="/images/logo.png"
         width="50"
         height="50"
         alt="Logo">
  </a>

  <ul class="nav-links">
    
    <li><a href="/about/">About</a></li>
    
    <li><a href="/graphpost/">Graph</a></li>
    
    <li><a href="/mlpost/">Machine Learning</a></li>
    
    <li><a href="/aipost/">Artificial Intelligence</a></li>
    
    <li><a href="/tspost/">Time Series</a></li>
    
    <li><a href="/theorypost/">Theory</a></li>
    
  </ul>
</nav>

      </header>


<main class="content" role="main">

  <article class="article">
    

    <h1 class="article-title">NLP Step by Step</h1>

    

    <div class="article-content">
      


<style>
body {
text-align: justify}
</style>
<p>This post has the aim to shows all the processes related to the NLP and how to use the Naive Bayes Classifier using Python and the <strong>nltk</strong> library.
We use data from spam detection.</p>
<p>In NLP a large part of the processing is Feature Engeneering. Tipically the steps are: </br>
<strong>Regular Expression</strong>: that is a formal language for specifying text strings: for example, we can have for the same word the s for plural, the capital first letter and any combination of those. We need a methodology to deal with this. </br>
<strong>Word Tokenization</strong>: every language processing has to minimize the text in some way, and we have to start with segmentation also known as tokenization of the words.
We have to find the <strong>lemma</strong> that is same <strong>stem</strong> or part of the word with the same root. Ones we tokenized our words we need to normalize that words, and also, we have to stem that word in order to reduce inflected words. Moreover, we have to reduce the uppercase in lowercase, with only some exceptions like: General Motors or FED for Federal Reserve Bank, that could be especially helpful in machine translation. </br>
<strong>Minimum Edit Distance</strong>: is the way of solving string similarities. It is ery helpful to correct grammar errors. So, the Minimum Edit Distance between two strings is the minimum number of editing operations (insertion, deletion, substitution) that are needed to transform one string into the other. We often need to use the so called <strong>Backtrace Method</strong>: similar to <a href="https://en.wikipedia.org/wiki/Dynamic_time_warping"><strong>DTW</strong></a> to find the minimum distance between two words. </br>
<strong>Object Standardization</strong>: in a text, are often present words or phrases which are not present in a standard lexiconsuch as acronyms, hashtags, colloquial slang. These are very present on social media. We can decide to include them in the stop words list. </br>
<strong>Bag of words</strong>: is used to extract features. We simply count the number of occurrences for each word, this process also called <strong>CountVectorizer</strong>. To make the CountVectorizer more comparable, we scale it using the Term Frequency Transformation <strong>TF</strong>, and in order to boost the most important features we use the Inverse Document Frequency <strong>IDF</strong>, this calculate how often a word occurs in the corpus. The combination of both is called <strong>TFiDF</strong> = TF * IDF. </br></p>
<p><span class="math display">\[
\begin{aligned} t f \cdot d t(t, D) &amp;=t f(t, d) \cdot d f(t, D) \\ t f(t, d) &amp;=f_{t \mu} \\ i d f(t, D) &amp;=\log \left(\frac{N}{|\{d \in D: t \in A\}|}\right) \end{aligned}
\]</span>
<strong>TFiDF</strong> is the LOG of the total number of documents N divided by the number of documents that contain the term that we are taking into consideration.
From the expression above, we have <strong>D</strong> the toal number of documents, and <strong>t</strong> number of documents with the term.
Matematically we can use the expression reported here below.</p>
<center>
<p><img src="/img/tfidfnlp.png" style="width:40.0%" /></p>
</center>
<p>The expression above determ the word <strong>Wx</strong> in the document <strong>y</strong>. The formula above give us not only the conut, but also the notation of the importance of a word in the entire corpus of all documents. </br>
Now we start with some explortatory data analysis and feature engineering using Python.</p>
<pre class="python"><code>messages.groupby(&#39;label&#39;).describe() # most popular ham and spam messages</code></pre>
<pre><code>##       message                                                               
##         count unique                                                top freq
## label                                                                       
## ham      4825   4516                             Sorry, I&#39;ll call later   30
## spam      747    653  Please call our customer service representativ...    4</code></pre>
<pre class="python"><code>messages[&#39;length&#39;] = messages[&#39;message&#39;].apply(len) # length of the messages
messages.head()</code></pre>
<pre><code>##   label                                            message  length
## 0   ham  Go until jurong point, crazy.. Available only ...     111
## 1   ham                      Ok lar... Joking wif u oni...      29
## 2  spam  Free entry in 2 a wkly comp to win FA Cup fina...     155
## 3   ham  U dun say so early hor... U c already then say...      49
## 4   ham  Nah I don&#39;t think he goes to usf, he lives aro...      61</code></pre>
<pre class="python"><code>messages[&#39;length&#39;].describe()</code></pre>
<pre><code>## count    5572.000000
## mean       80.489950
## std        59.942907
## min         2.000000
## 25%        36.000000
## 50%        62.000000
## 75%       122.000000
## max       910.000000
## Name: length, dtype: float64</code></pre>
<p>From the code above we can see we have a etxt with 910 characters. Here below the text of the message.</p>
<pre class="python"><code>messages[messages[&#39;length&#39;] == 910][&#39;message&#39;].iloc[0]</code></pre>
<pre><code>## &quot;For me the love should start with attraction.i should feel that I need her every time around me.she should be the first thing which comes in my thoughts.I would start the day and end it with her.she should be there every time I dream.love will be then when my every breath has her name.my life should happen around her.my life will be named to her.I would cry for her.will give all my happiness and take all her sorrows.I will be ready to fight with anyone for her.I will be in love when I will be doing the craziest things for her.love will be when I don&#39;t have to proove anyone that my girl is the most beautiful lady on the whole planet.I will always be singing praises for her.love will be when I start up making chicken curry and end up makiing sambar.life will be the most beautiful then.will get every morning and thank god for the day because she is with me.I would like to say a lot..will tell later..&quot;</code></pre>
<p>There are many methods to convert a corpus of strings to a vector format and the simplest way is usiing <strong>bag of words</strong> where each unique word in the text will represented by one vector. The first step is to remove punctuation, and stopwords.</p>
<pre class="python"><code>import string
from nltk.corpus import stopwords

def text_process(mess):
    &quot;&quot;&quot;
    Takes in a string of text, then performs the following:
    1. Remove all punctuation
    2. Remove all stopwords
    3. Returns a list of the cleaned text
    &quot;&quot;&quot;
    # Check characters to see if they are in punctuation
    nopunc = [char for char in mess if char not in string.punctuation]

    # Join the characters again to form the string.
    nopunc = &#39;&#39;.join(nopunc)
    
    # Now just remove any stopwords
    return [word for word in nopunc.split() if word.lower() not in stopwords.words(&#39;english&#39;)]
    
# Check it
messages[&#39;message&#39;].head(5).apply(text_process)
    </code></pre>
<pre><code>## 0    [Go, jurong, point, crazy, Available, bugis, n...
## 1                       [Ok, lar, Joking, wif, u, oni]
## 2    [Free, entry, 2, wkly, comp, win, FA, Cup, fin...
## 3        [U, dun, say, early, hor, U, c, already, say]
## 4    [Nah, dont, think, goes, usf, lives, around, t...
## Name: message, dtype: object</code></pre>
<p>Now, we can focus on vectorization namely count how many times a word occur in a text, and this is called <strong>Term Frequency</strong>. We have also to <strong>normalize</strong> the vector to unit length. Since there are so many messages, we can expect lot of zeros. Because of this, we can use <strong>SciKit Learn</strong> to otput a <a href="https://en.wikipedia.org/wiki/Sparse_matrix"><strong>Sparse Matrix</strong></a>.</p>
<pre class="python"><code>from sklearn.feature_extraction.text import CountVectorizer

# Create bag of words
bow_transformer = CountVectorizer(analyzer=text_process).fit(messages[&#39;message&#39;])

# Check the sparsity
messages_bow = bow_transformer.transform(messages[&#39;message&#39;])
print(&#39;Shape of Sparse Matrix: &#39;, messages_bow.shape)</code></pre>
<pre><code>## Shape of Sparse Matrix:  (5572, 11425)</code></pre>
<pre class="python"><code>print(&#39;Amount of Non-Zero occurences: &#39;, messages_bow.nnz)</code></pre>
<pre><code>## Amount of Non-Zero occurences:  50548</code></pre>
<pre class="python"><code>sparsity = (100.0 * messages_bow.nnz / (messages_bow.shape[0] * messages_bow.shape[1]))
print(&#39;sparsity: {}&#39;.format(sparsity))</code></pre>
<pre><code>## sparsity: 0.07940295412668218</code></pre>
<p>Now that we have an idea about the sparsity of of bag of word, we can</p>

    </div>
  </article>

  


</main>

      <footer class="footer">
        <ul class="footer-links">
          <li>
            <a href="/index.xml" type="application/rss+xml" target="_blank">RSS feed</a>
          </li>
          <li>
            <a href="https://www.rstudio.com/" class="footer-links-kudos">Made with <img src="/images/hugo-logo.png" alt="Img link to Hugo website" width="22" height="22"></a>
          </li>
        </ul>
      </footer>

    </div>
    



<script src="//cdnjs.cloudflare.com/ajax/libs/highlight.js/9.12.0/highlight.min.js"></script>



<script src="//cdnjs.cloudflare.com/ajax/libs/highlight.js/9.12.0/languages/r.min.js"></script>
<script src="//cdnjs.cloudflare.com/ajax/libs/highlight.js/9.12.0/languages/yaml.min.js"></script>
<script>hljs.configure({languages: []}); hljs.initHighlightingOnLoad();</script>



    
<script src="/js/math-code.js"></script>
<script async src="//cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.5/MathJax.js?config=TeX-MML-AM_CHTML"></script>


    
<script type="application/javascript">
var doNotTrack = false;
if (!doNotTrack) {
	(function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
	(i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
	m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
	})(window,document,'script','https://www.google-analytics.com/analytics.js','ga');
	ga('create', 'UA-51254710-89', 'auto');
	
	ga('send', 'pageview');
}
</script>

  </body>
</html>

