<?xml version="1.0" encoding="utf-8" standalone="yes" ?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title>artificial intelligence on Andrea Perlato</title>
    <link>/categories/artificial-intelligence/</link>
    <description>Recent content in artificial intelligence on Andrea Perlato</description>
    <generator>Hugo -- gohugo.io</generator>
    <language>en-us</language>
    <lastBuildDate>Mon, 25 Nov 2019 00:00:00 +0000</lastBuildDate>
    
	<atom:link href="/categories/artificial-intelligence/index.xml" rel="self" type="application/rss+xml" />
    
    
    <item>
      <title>Root Mean Square Propagation</title>
      <link>/aipost/root-mean-square-propagation/</link>
      <pubDate>Mon, 25 Nov 2019 00:00:00 +0000</pubDate>
      
      <guid>/aipost/root-mean-square-propagation/</guid>
      <description>body {text-align: justify}The Root Mean Square Propagation RMS Prop is similar to Momentum, it is a technique to dampen out the motion in the y-axis and speed up gradient descent.For better understanding, let us denote the Y-axis as the bias b and the X-axis as the weight W.
The intuition is that when we divide a large number by another number, the result becomes small.</description>
    </item>
    
    <item>
      <title>Gradient Checking</title>
      <link>/aipost/gradient-checking/</link>
      <pubDate>Fri, 22 Nov 2019 00:00:00 +0000</pubDate>
      
      <guid>/aipost/gradient-checking/</guid>
      <description>body {text-align: justify}When we implement backpropagation there is a test called Gradient Checking that helps to make sure that the implementation of backpropagation is correct.
Looking at the figure above we can get much better estimate of gradient if we use a larger approximation of the derivative using a double triangle.The hiight of the triagle in the figure can be seen as follow:</description>
    </item>
    
    <item>
      <title>Gradient Descent with Momentum</title>
      <link>/aipost/gradient-descent-with-momentum/</link>
      <pubDate>Fri, 22 Nov 2019 00:00:00 +0000</pubDate>
      
      <guid>/aipost/gradient-descent-with-momentum/</guid>
      <description>body {text-align: justify}Gradient Descent with momentum or just Momentum is an advanced optimization algorithm that speeds up the optimization of the cost function J. It makes use of the moving average to update the trainable parameters of the neural network. Moving average is the average calculated over n successive values rather than the whole set of values. Mathematically, it is denoted as follow:
\[A_{t}=\beta A_{t-1}+(1-\beta) X_{t}\]</description>
    </item>
    
    <item>
      <title>Mini-batch Gradient Descent</title>
      <link>/aipost/mini-batch-gradient-descent/</link>
      <pubDate>Fri, 22 Nov 2019 00:00:00 +0000</pubDate>
      
      <guid>/aipost/mini-batch-gradient-descent/</guid>
      <description>body {text-align: justify}In Batch Gradient Descent on every interation we go through the entire training set.From the figure below we can see the cost function J on the left a batch gradient descent that decrease every single interation. On the right we have the cost function J of a mini-batch gradient descent where in every interatin our processing in training on a different train-set; that is why the loss function J is going to be a little noisier.</description>
    </item>
    
    <item>
      <title>Vanishing Gradient</title>
      <link>/aipost/vanishing-gradient/</link>
      <pubDate>Fri, 22 Nov 2019 00:00:00 +0000</pubDate>
      
      <guid>/aipost/vanishing-gradient/</guid>
      <description>body {text-align: justify}One of the problems of training a deep neural network is the vanishing and exploding gradient: when we train a deep network the derivatives or the slope can get very big or very small or exponentially small and this makes training difficult. We have to choose very carefully the random weight initialization in order to avoid this problem.
\[\omega^{[l]}=\left[\begin{array}{cc}{1.5} &amp;amp; {0} \\ {0} &amp;amp; {1.</description>
    </item>
    
    <item>
      <title>Vectorization in Python</title>
      <link>/aipost/vectorization-in-python/</link>
      <pubDate>Fri, 18 Oct 2019 00:00:00 +0000</pubDate>
      
      <guid>/aipost/vectorization-in-python/</guid>
      <description>body {text-align: justify}In deep learing we often deal with large data sets. It is important to run the code quickly because otherwise the code might take a long time to get the results.That is why perform vectorization has become a key skill.
For example in logistic regression we need to to compute w transpose x in a non-vectorized implementation we can use the following code:</description>
    </item>
    
    <item>
      <title>Introduction to Word2Vec</title>
      <link>/theorypost/introduction-to-word2vec/</link>
      <pubDate>Tue, 06 Aug 2019 00:00:00 +0000</pubDate>
      
      <guid>/theorypost/introduction-to-word2vec/</guid>
      <description>body {text-align: justify}The Word2Vec is a deep learning algorithm that draws context from phrases. Every textual document is represented in the form of a vector, and that is done through Vector Space Modelling (VSM). We can convert our text using One Hot Encoding. For example, having three words we can make a vector in a three dimentional space.
The problem with One Hot Encoding is that it doesn’t help us to find similarities.</description>
    </item>
    
    <item>
      <title>Predict Movie Sentiment via DOC2VEC</title>
      <link>/aipost/predict-movie-sentiment-via-doc2vec/</link>
      <pubDate>Tue, 06 Aug 2019 00:00:00 +0000</pubDate>
      
      <guid>/aipost/predict-movie-sentiment-via-doc2vec/</guid>
      <description>body {text-align: justify}In order to have an introduction of the Word2Vec look at this post.Using this method we try to predict the movie sentiment (positive vs. negative) using text data as predictor. We use the movie review data set, and we use the power of Doc2Vec to transform our data into predictors.
library(text2vec)library(tm)data(movie_review)names(movie_review)[1] &amp;quot;id&amp;quot; &amp;quot;sentiment&amp;quot; &amp;quot;review&amp;quot; The data set contain an id, sentiment (0=negative, 1=positive), and review that contains the text if people like the movie or not.</description>
    </item>
    
    <item>
      <title>Auto Encoder</title>
      <link>/aipost/auto-encoder/</link>
      <pubDate>Mon, 29 Jul 2019 00:00:00 +0000</pubDate>
      
      <guid>/aipost/auto-encoder/</guid>
      <description>body {text-align: justify}It encodes itself using Visible Input Nodes, and the Visible Output Nodes are decoded using Hidden Nodes, in order to be identical to the Input Nodes. It is not a pure Unsupervised Deep Learning algorithm, but it is a Self-Supervised Deep Learning algorithm.
Auto Encoders can be used for Feature Detection. Once we have encoded our data, the Hidden Nodes also called Encoder Nodes, will be represent certain features which are important in ur data.</description>
    </item>
    
    <item>
      <title>Boltzmann Machine</title>
      <link>/aipost/boltzmann-machine/</link>
      <pubDate>Sun, 28 Jul 2019 00:00:00 +0000</pubDate>
      
      <guid>/aipost/boltzmann-machine/</guid>
      <description>body {text-align: justify}Boltzmann Machine is an Unsupervised Deep Learning used for Recommendation System. Boltzmann Machines are undirected models, they don’t have a direction in the connections as described in the figure below.
From the figure above, we can see that there is not an output layer. This makes Boltzmann Machine fundamentally different to all other algorithms, and it doesn’t expect input data, but it generates information regardless of input nodes.</description>
    </item>
    
    <item>
      <title>CNN and Softmax</title>
      <link>/aipost/cnn-and-softmax/</link>
      <pubDate>Tue, 23 Jul 2019 00:00:00 +0000</pubDate>
      
      <guid>/aipost/cnn-and-softmax/</guid>
      <description>body {text-align: justify}Convolutional neural network CNN is a Supervised Deep Learning used for Computer Vision. The process of Convolutional Neural Networks can be devided in five steps: Convolution, Max Pooling, Flattening, Full Connection.
STEP 1 - Convolution At the bases of Convolution there is a filter also called Feature Detector or Kernel. We basically multiply the portion of the image by the filter and we check the matching how many 1s have in common.</description>
    </item>
    
    <item>
      <title>Self Organizing Maps</title>
      <link>/aipost/self-organizing-maps/</link>
      <pubDate>Tue, 23 Jul 2019 00:00:00 +0000</pubDate>
      
      <guid>/aipost/self-organizing-maps/</guid>
      <description>body {text-align: justify}SOM is an Unsupervised Deep Learning used for Feature Detection. SOMs are great for dimensionality reduction.
They take a multidimensional data set with lots of columns and end up with a map in a two-dimensional representation using an unsupervised algorithm.It is a similar approach like the K-Mean Clustering.
How SOMs Learn: Best Matching Unit BMU The weights in SOMs are different.</description>
    </item>
    
    <item>
      <title>Recurrent Neural Network in Theory</title>
      <link>/aipost/recurrent-neural-network-in-theory/</link>
      <pubDate>Fri, 19 Jul 2019 00:00:00 +0000</pubDate>
      
      <guid>/aipost/recurrent-neural-network-in-theory/</guid>
      <description>body {text-align: justify}RNN is a Supervised Deep Learning used for Time Series Analysis. Recurrent Neural Networks represent one of the most advanced algorithms that exist in the world of supervised deep learning. 
Frontal lobe and RNN RNNs are like short-term memory. We will learn that they can remember things that just happen in a previous couple of observations and apply that knowledge in the going forward.</description>
    </item>
    
    <item>
      <title>Alpha Beta Pruning</title>
      <link>/aipost/alpha-beta-pruning/</link>
      <pubDate>Thu, 20 Jun 2019 00:00:00 +0000</pubDate>
      
      <guid>/aipost/alpha-beta-pruning/</guid>
      <description>body {text-align: justify}Introduction  A fascinating aspect of our brain is the Synaptic pruning. One of the grand strategies nature uses to construct nervous systems is to overproduce neural elements, such as neurons, axons and synapses, and then prune the excess. In fact, this overproduction is so substantial that only about half of the neurons mammalian embryos generate will survive until birth. At the same way the pruning in ANN is used to eliminate redundant connections between neurons during the training.</description>
    </item>
    
    <item>
      <title>The Activation Function</title>
      <link>/aipost/the-activation-function/</link>
      <pubDate>Thu, 20 Jun 2019 00:00:00 +0000</pubDate>
      
      <guid>/aipost/the-activation-function/</guid>
      <description>body {text-align: justify}What an artifical neuron do is to calculate a weighted sum of its input, adds a bias and then decides whether it should be “fired” or not.
considering the neuron of the figure above, the value of Y can be anything ranging from -inf to +inf. The neuron really doesn’t know the bounds of the value. How do we decide whether the neuron should fire or not?</description>
    </item>
    
    <item>
      <title>Introduction to Convolutional Neural Networks</title>
      <link>/aipost/introduction-to-convolutional-neural-networks/</link>
      <pubDate>Tue, 18 Jun 2019 00:00:00 +0000</pubDate>
      
      <guid>/aipost/introduction-to-convolutional-neural-networks/</guid>
      <description>body {text-align: justify}Convolutional Neural Networks is one of the most succesfully and used Neural Network Algorithm. The three main components of the CNN are the Convolutional Layer, the Pooling Layer (used to reduce the computational space), and the Fully Connected Layer.  Image to have to classify 32x32 images. A single Fully-Connected Neuron in a first hidden layer would have 3131x3=3072 weights and this structure can not scale to larger images.</description>
    </item>
    
    <item>
      <title>Backpropagation Intuition</title>
      <link>/aipost/backpropagation/</link>
      <pubDate>Mon, 17 Jun 2019 00:00:00 +0000</pubDate>
      
      <guid>/aipost/backpropagation/</guid>
      <description>body {text-align: justify}Introduction  We already know that there is in ANN a Forwward Propagation where the information is entered into the input layer, and then it is propagated forward to get our output values to compare with the actual values that we have in our training set, and then we calculate the errors. Then the errors are back propagated through the network in the opposite direction in order to adjust the weights.</description>
    </item>
    
    <item>
      <title>Stochastic Gradient Descent</title>
      <link>/aipost/stochastic-gradient-descent/</link>
      <pubDate>Tue, 11 Jun 2019 00:00:00 +0000</pubDate>
      
      <guid>/aipost/stochastic-gradient-descent/</guid>
      <description>body { text-align: justify}  This article is a summary of the StatQuest video made by Josh Starmer.  Click here to see the video explained by Josh Starmer. 
In Gradient Descent, we used the sum of the squared residuals as the Loss Function to determine how well the initial line fit the data. Than, we took the derivative of the sum of the squared residuals with respect to the intercept and slope.</description>
    </item>
    
    <item>
      <title>The Learning Process</title>
      <link>/aipost/the-learning-process/</link>
      <pubDate>Mon, 20 May 2019 00:00:00 +0000</pubDate>
      
      <guid>/aipost/the-learning-process/</guid>
      <description>body {text-align: justify}Learning means to generalize what we learned and improve the performance of the same task based on a given measure. More specifically, it means to adjusting the parameters of the model in order to accurately predict the dependent variables on new input data. More formally we can define two main functions: the Score Function and the Loss Function.  The score Function describes our mapping from the input space x to the output space y.</description>
    </item>
    
    <item>
      <title>Neural Nets and Interactive Graphs</title>
      <link>/tspost/neural-nets-and-interactive-graphs/</link>
      <pubDate>Wed, 27 Feb 2019 00:00:00 +0000</pubDate>
      
      <guid>/tspost/neural-nets-and-interactive-graphs/</guid>
      <description>body {text-align: justify}In this post, we use some fairly new technology of time series analysis namely neural nets and interactive charting tools. These techniques are the state of the art. The dataset we use for this example has: missing data, outliers, poor formatting.  The dataset i about restaurant at a campsite that is open whole year. There is a peak season in summer, and so we aspect to have seasonal data and trend might be present.</description>
    </item>
    
    <item>
      <title>Distinguish Benign and Malign Tumor via ANN</title>
      <link>/aipost/distinguish-benign-and-malign-tumor-via-ann/</link>
      <pubDate>Tue, 05 Feb 2019 00:00:00 +0000</pubDate>
      
      <guid>/aipost/distinguish-benign-and-malign-tumor-via-ann/</guid>
      <description>body {text-align: justify}We try to recognize cancer in human breast using a multi-hidden layer artificial neural network via H2O package. We use the Wisconsin Breast-Cancer Dataset which is a collectioin of Dr.Wolberg real clinical cases. There are no images, but we can recognize malignal tumor based on 10 biomedical attributes. We have a total number of 699 patients divided in two classes: malignal and benign cancer.</description>
    </item>
    
    <item>
      <title>How Neural Network learn? An example of risk of churn.</title>
      <link>/aipost/how-neural-network-learn/</link>
      <pubDate>Mon, 14 Jan 2019 00:00:00 +0000</pubDate>
      
      <guid>/aipost/how-neural-network-learn/</guid>
      <description>body {text-align: justify}Having a one layer neural network (single layer feedforeward) with the output value to be compare to the actual value. Baed on the activation function we have our output. In order to be able to lear, we have to compare the output value with the actual value via the cost funtion which is the half of the squred difference output and actual value.</description>
    </item>
    
  </channel>
</rss>