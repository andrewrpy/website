<?xml version="1.0" encoding="utf-8" standalone="yes" ?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title>machine learning on Andrea Perlato</title>
    <link>/categories/machine-learning/</link>
    <description>Recent content in machine learning on Andrea Perlato</description>
    <generator>Hugo -- gohugo.io</generator>
    <language>en-us</language>
    <lastBuildDate>Sat, 04 Apr 2020 00:00:00 +0000</lastBuildDate>
    
	<atom:link href="/categories/machine-learning/index.xml" rel="self" type="application/rss+xml" />
    
    
    <item>
      <title>Cloud Services Video Interview</title>
      <link>/theorypost/cloud-servises-video-interview/</link>
      <pubDate>Sat, 04 Apr 2020 00:00:00 +0000</pubDate>
      
      <guid>/theorypost/cloud-servises-video-interview/</guid>
      <description>body {text-align: justify}.rwd-video {height: 0;overflow: hidden;padding-bottom: 56.25%;padding-top: 30px;position: relative;}.rwd-video iframe,.rwd-video object,.rwd-video embed {height: 100%;left: 0;position: absolute;top: 0;width: 100%;}</description>
    </item>
    
    <item>
      <title>Feature Engineering Video Interview</title>
      <link>/theorypost/feature-engineering-video-interview/</link>
      <pubDate>Sat, 04 Apr 2020 00:00:00 +0000</pubDate>
      
      <guid>/theorypost/feature-engineering-video-interview/</guid>
      <description>body {text-align: justify}.rwd-video {height: 0;overflow: hidden;padding-bottom: 56.25%;padding-top: 30px;position: relative;}.rwd-video iframe,.rwd-video object,.rwd-video embed {height: 100%;left: 0;position: absolute;top: 0;width: 100%;}</description>
    </item>
    
    <item>
      <title>Feature Selection Video Interview</title>
      <link>/theorypost/feature-selection-video-interview/</link>
      <pubDate>Sat, 04 Apr 2020 00:00:00 +0000</pubDate>
      
      <guid>/theorypost/feature-selection-video-interview/</guid>
      <description>body {text-align: justify}.rwd-video {height: 0;overflow: hidden;padding-bottom: 56.25%;padding-top: 30px;position: relative;}.rwd-video iframe,.rwd-video object,.rwd-video embed {height: 100%;left: 0;position: absolute;top: 0;width: 100%;}</description>
    </item>
    
    <item>
      <title>Machine Learning Algorithms Video Interview</title>
      <link>/theorypost/machine-learning-algorithms-video-interview/</link>
      <pubDate>Sat, 04 Apr 2020 00:00:00 +0000</pubDate>
      
      <guid>/theorypost/machine-learning-algorithms-video-interview/</guid>
      <description>body {text-align: justify}.rwd-video {height: 0;overflow: hidden;padding-bottom: 56.25%;padding-top: 30px;position: relative;}.rwd-video iframe,.rwd-video object,.rwd-video embed {height: 100%;left: 0;position: absolute;top: 0;width: 100%;}</description>
    </item>
    
    <item>
      <title>Machine Learning Pipeline Video Interview</title>
      <link>/theorypost/machine-learning-pipeline-video-interview/</link>
      <pubDate>Sat, 04 Apr 2020 00:00:00 +0000</pubDate>
      
      <guid>/theorypost/machine-learning-pipeline-video-interview/</guid>
      <description>body {text-align: justify}.rwd-video {height: 0;overflow: hidden;padding-bottom: 56.25%;padding-top: 30px;position: relative;}.rwd-video iframe,.rwd-video object,.rwd-video embed {height: 100%;left: 0;position: absolute;top: 0;width: 100%;}</description>
    </item>
    
    <item>
      <title>Model Deployment Video Interview</title>
      <link>/theorypost/model-deployment-video-interview/</link>
      <pubDate>Sat, 04 Apr 2020 00:00:00 +0000</pubDate>
      
      <guid>/theorypost/model-deployment-video-interview/</guid>
      <description>body {text-align: justify}.rwd-video {height: 0;overflow: hidden;padding-bottom: 56.25%;padding-top: 30px;position: relative;}.rwd-video iframe,.rwd-video object,.rwd-video embed {height: 100%;left: 0;position: absolute;top: 0;width: 100%;}</description>
    </item>
    
    <item>
      <title>Model Performance Video Interview</title>
      <link>/theorypost/model-performance-video-interview/</link>
      <pubDate>Sat, 04 Apr 2020 00:00:00 +0000</pubDate>
      
      <guid>/theorypost/model-performance-video-interview/</guid>
      <description>body {text-align: justify}.rwd-video {height: 0;overflow: hidden;padding-bottom: 56.25%;padding-top: 30px;position: relative;}.rwd-video iframe,.rwd-video object,.rwd-video embed {height: 100%;left: 0;position: absolute;top: 0;width: 100%;}</description>
    </item>
    
    <item>
      <title>Natural Language Processing Video Interview</title>
      <link>/theorypost/natural-language-processing-video-interview/</link>
      <pubDate>Sat, 04 Apr 2020 00:00:00 +0000</pubDate>
      
      <guid>/theorypost/natural-language-processing-video-interview/</guid>
      <description>body {text-align: justify}.rwd-video {height: 0;overflow: hidden;padding-bottom: 56.25%;padding-top: 30px;position: relative;}.rwd-video iframe,.rwd-video object,.rwd-video embed {height: 100%;left: 0;position: absolute;top: 0;width: 100%;}</description>
    </item>
    
    <item>
      <title>R and Python Video Interview</title>
      <link>/theorypost/r-and-python/</link>
      <pubDate>Sat, 04 Apr 2020 00:00:00 +0000</pubDate>
      
      <guid>/theorypost/r-and-python/</guid>
      <description>body {text-align: justify}.rwd-video {height: 0;overflow: hidden;padding-bottom: 56.25%;padding-top: 30px;position: relative;}.rwd-video iframe,.rwd-video object,.rwd-video embed {height: 100%;left: 0;position: absolute;top: 0;width: 100%;}</description>
    </item>
    
    <item>
      <title>Requirements to Deploy Video Interview</title>
      <link>/theorypost/requirements-to-deply-video-interview/</link>
      <pubDate>Sat, 04 Apr 2020 00:00:00 +0000</pubDate>
      
      <guid>/theorypost/requirements-to-deply-video-interview/</guid>
      <description>body {text-align: justify}.rwd-video {height: 0;overflow: hidden;padding-bottom: 56.25%;padding-top: 30px;position: relative;}.rwd-video iframe,.rwd-video object,.rwd-video embed {height: 100%;left: 0;position: absolute;top: 0;width: 100%;}</description>
    </item>
    
    <item>
      <title>Semantic Analysis</title>
      <link>/theorypost/semantic-analysis/</link>
      <pubDate>Sat, 04 Apr 2020 00:00:00 +0000</pubDate>
      
      <guid>/theorypost/semantic-analysis/</guid>
      <description>body {text-align: justify}.rwd-video {height: 0;overflow: hidden;padding-bottom: 56.25%;padding-top: 30px;position: relative;}.rwd-video iframe,.rwd-video object,.rwd-video embed {height: 100%;left: 0;position: absolute;top: 0;width: 100%;}</description>
    </item>
    
    <item>
      <title>Ensemble Learning with Adaptive Boosting</title>
      <link>/theorypost/ensemble-learning-with-adaptive-boosting/</link>
      <pubDate>Fri, 03 Jan 2020 00:00:00 +0000</pubDate>
      
      <guid>/theorypost/ensemble-learning-with-adaptive-boosting/</guid>
      <description>body {text-align: justify}Why Ensemble First of all, ensemble often have lower error than any individual method by themselves. It has also less overfitting leading us to a better performance in the test set. Each kind of learners that we might use has a sort of bias. Combining all of them, can reduce this bias. 
Adaptive Boosting in Random Forest In a Forest of Trees made with ADA Boost the trees are usually just One Node with Two Leaves that are called Stump, so it is a Forest of Stump.</description>
    </item>
    
    <item>
      <title>Ensemble Learning with Gradient Boosting</title>
      <link>/theorypost/ensemble-learning-with-gradient-boosting/</link>
      <pubDate>Fri, 03 Jan 2020 00:00:00 +0000</pubDate>
      
      <guid>/theorypost/ensemble-learning-with-gradient-boosting/</guid>
      <description>body {text-align: justify}Ensemble often have lower error than any individual method by themselves.It has also less overfitting leading us to a better performance in the test set.Each kind of learners that we might use has a sort of bias. Combining all of them, can reduce this bias. 
Gradient Boost vs. AdaBoost We start considering to variable reported int he figure below, and we want to predict weight from these variables using decision tree.</description>
    </item>
    
    <item>
      <title>Exploratory Data Analysis</title>
      <link>/mlpost/exploratory-data-analysis/</link>
      <pubDate>Tue, 17 Dec 2019 00:00:00 +0000</pubDate>
      
      <guid>/mlpost/exploratory-data-analysis/</guid>
      <description>body {text-align: justify}Before the definition of the model it is always important to plot the distribution of our variable, finding unusual observation (points of leverage, hat value and outliers), missing data (understand if they are missing at random or not at random). Then, I always put the data in a multiple regression to see correlations and the presence of multicollinearity: is the phenomenon in which one predictor can be linearly predict from others with a substantial degree of accuracy.</description>
    </item>
    
    <item>
      <title>Azure Databricks and RStudio</title>
      <link>/mlpost/azure-databricks-and-rstudio/</link>
      <pubDate>Wed, 20 Nov 2019 00:00:00 +0000</pubDate>
      
      <guid>/mlpost/azure-databricks-and-rstudio/</guid>
      <description>body {text-align: justify}In the analytics market Spark is taking off for ETL and Machine Learning.Azure Databircks is a managed version of Spark and very quickly a data scientst is able to start from zero and get to insights. Moreover, in addition it is intereget with Azure AD in order to use the same authentication model that is used for every other services of Azure. A very interesting topic is the integration of RStudio with Azure Databricks and Spark.</description>
    </item>
    
    <item>
      <title>Bias Variance Trade-Off</title>
      <link>/theorypost/bias-variance-trade-off/</link>
      <pubDate>Thu, 07 Nov 2019 00:00:00 +0000</pubDate>
      
      <guid>/theorypost/bias-variance-trade-off/</guid>
      <description>body {text-align: justify}The Bias Variance Trade-Off is used to understand the model’s performance and evaluation. We we have a training error that goes down, nut test error starting to go up, the model we created begins to overfit.
Image to have a Linear Regression ML, but is not accurate to replicate the curve of the true relationship between height and weight.The inability for an ML to capture the true relationship is called bias.</description>
    </item>
    
    <item>
      <title>Introduction to Support Vector Machine</title>
      <link>/theorypost/introduction-to-support-vector-machine/</link>
      <pubDate>Wed, 02 Oct 2019 00:00:00 +0000</pubDate>
      
      <guid>/theorypost/introduction-to-support-vector-machine/</guid>
      <description>body {text-align: justify}This article is a summary of the StatQuest video made by Josh Starmer. Click here to see the video explained by Josh Starmer. 
We use as an example the measurement of the Mass of mice (g). The red dots in the figure below represent mice that are not obese and the green dots represent mice obese. Based on this observation, we can pick a threshold, and when we have a new observation that has less mass than the threshold we can classify it as not obese.</description>
    </item>
    
    <item>
      <title>Extract the Main Topics from Books</title>
      <link>/mlpost/extract-the-main-topics-from-books/</link>
      <pubDate>Fri, 09 Aug 2019 00:00:00 +0000</pubDate>
      
      <guid>/mlpost/extract-the-main-topics-from-books/</guid>
      <description>body {text-align: justify}Topic modeling is the process of identifying topics in a set of documents. This can be useful for search engines, customer service automation, and any other instance where knowing the topics of documents is important. There are multiple methods of going about doing this. The most commonly used is Latent Dirichlet Allocation.The LDA builds a topic per document model and words per topic model, modeled as Dirichlet distributions.</description>
    </item>
    
    <item>
      <title>Introduction to Naive Bayes</title>
      <link>/theorypost/introduction-to-naive-bayes/</link>
      <pubDate>Tue, 06 Aug 2019 00:00:00 +0000</pubDate>
      
      <guid>/theorypost/introduction-to-naive-bayes/</guid>
      <description>body {text-align: justify}It is a Probability Classifier. Naïve Bayes is the first algorithm that should be considered for solving Text Classification Problem which involves High Dimensional training Dataset. A few examples are: Sentiment Analysis and Classifying Topics on Social Media. It also refers to the Bayes’ Theorem also known as Bayes’ Law that give us a method to calculate the Conditional Probability: that is the probability of an event, based on previous knowledge available on the events.</description>
    </item>
    
    <item>
      <title>Identify Spam Emails</title>
      <link>/mlpost/identify-smap-emails/</link>
      <pubDate>Fri, 02 Aug 2019 00:00:00 +0000</pubDate>
      
      <guid>/mlpost/identify-smap-emails/</guid>
      <description>body {text-align: justify}We want to differentiate between spam (called spam) and non-spam (called ham) email based on the content. We use a training set of textual data that are already labeled spam/non-spam email.
We start removing empy columns, and we call our columns label and text.We also create a corpus, remove punctuation, transform everything into lowercase, remove numbers, and stop words. Then, we have to stamming the document, and finally we have a corpus of terms.</description>
    </item>
    
    <item>
      <title>Introduction to K-Means and Hierarchical clustering</title>
      <link>/theorypost/introduction-to-k-means-and-hierarchical-clustering/</link>
      <pubDate>Fri, 02 Aug 2019 00:00:00 +0000</pubDate>
      
      <guid>/theorypost/introduction-to-k-means-and-hierarchical-clustering/</guid>
      <description>body {text-align: justify}This article is a summary of the StatQuest video made by Josh Starmer. Click here to see the video explained by Josh Starmer. 
K-Means Clustering This is a popular unsupervised machine learning algorithms. The objective of K-means is simple: group similar data points together and discover underlying patterns. The Step - 1 is to identify the number of clusters. Suppose we have K=3, the Step - 2 is to select randomly 3 data points and these are our Initial Cluster Points.</description>
    </item>
    
    <item>
      <title>Ensemble Learning</title>
      <link>/theorypost/ensemble-learning/</link>
      <pubDate>Fri, 26 Jul 2019 00:00:00 +0000</pubDate>
      
      <guid>/theorypost/ensemble-learning/</guid>
      <description>body {text-align: justify}This article is a summary of the StatQuest video made by Josh Starmer. Click here to see the video explained by Josh Starmer. 
The Boosting and Ensemble Learning concepts can be applied to many Machine Learning models: it is a Meta Algorithm used to convert many weak learners into a strong learner in order to achieve good performance in supervised problems.</description>
    </item>
    
    <item>
      <title>Principal Component Analysis</title>
      <link>/theorypost/principal-component-analysis/</link>
      <pubDate>Fri, 26 Jul 2019 00:00:00 +0000</pubDate>
      
      <guid>/theorypost/principal-component-analysis/</guid>
      <description>body {text-align: justify}This article is a summary of the StatQuest video made by Josh Starmer. Click here to see the video explained by Josh Starmer. 
The Principal Component Analysis is a deterministic method (given an input will always produce the same output). It is always good to perform a PCA: Principal Components Analysis (PCA) is a data reduction technique that transforms a larger number of correlated variables into a much smaller set of uncorrelated variables called Principal Components.</description>
    </item>
    
    <item>
      <title>Synthetic Minority Oversampling Technique</title>
      <link>/theorypost/synthetic-minority-oversampling-technique/</link>
      <pubDate>Mon, 17 Jun 2019 00:00:00 +0000</pubDate>
      
      <guid>/theorypost/synthetic-minority-oversampling-technique/</guid>
      <description>body {text-align: justify}Oversampling and undersampling in data analysis are techniques used to adjust the class distribution of a data set. Both oversampling and undersampling involve introducing a bias to select more samples from one class than from another, to compensate for an imbalance that is either already present in the data. Data Imbalance can be of the following types:  Under-representation of a class in one or more important predictor variables.</description>
    </item>
    
    <item>
      <title>Time Series Classification</title>
      <link>/tspost/time-series-classification/</link>
      <pubDate>Fri, 05 Apr 2019 00:00:00 +0000</pubDate>
      
      <guid>/tspost/time-series-classification/</guid>
      <description>body {text-align: justify}In order to perform a Time series Classification we use Decision Tree, and then we look at the performance of the classification. 
We use the Synthetic Control Chart Time Series. This dataset contains 600 examples of control charts synthetically generated by the process in Alcock and Manolopoulos (1999).
data &amp;lt;- read.table(&amp;quot;C:/07 - R Website/dataset/TS/synthetic_control.txt&amp;quot;, header = FALSE)# Data Preparationpattern100 &amp;lt;- c(rep(&amp;#39;Normal&amp;#39;, 100),rep(&amp;#39;Cyclic&amp;#39;, 100),rep(&amp;#39;Increasing trend&amp;#39;, 100),rep(&amp;#39;Decreasing trend&amp;#39;, 100),rep(&amp;#39;Upward shift&amp;#39;, 100),rep(&amp;#39;Downward shift&amp;#39;, 100))# Create data framenewdata &amp;lt;- data.</description>
    </item>
    
    <item>
      <title>Linear Discriminant Analysis</title>
      <link>/mlpost/linear-discriminant-analysis/</link>
      <pubDate>Thu, 04 Apr 2019 00:00:00 +0000</pubDate>
      
      <guid>/mlpost/linear-discriminant-analysis/</guid>
      <description>body {text-align: justify}Linear Discriminant Analysis was originally developed by R.A. Fisher to classify subjects into one of the two clearly defined groups. It was later expanded to classify subjects inoto more than two groups. It helps to find linear combination of original variables that provide the best possible separation between the groups. Linear Discriminant Analysis is focused on maximizing the separability among known categories. The problem is when 2 features are not sufficient to capture the most of variation.</description>
    </item>
    
    <item>
      <title>Classification and Prediction with Support Vector Machine</title>
      <link>/mlpost/classification-and-prediction-with-support-vector-machine/</link>
      <pubDate>Wed, 03 Apr 2019 00:00:00 +0000</pubDate>
      
      <guid>/mlpost/classification-and-prediction-with-support-vector-machine/</guid>
      <description>body {text-align: justify}Support Vector Machine SVM is a linear classifier. We can consider SVM for linearly separable binary sets. The goal is to design a hyperplane (is a subspace whose dimension is one less than that of its ambient space. If a space is 3-dimensional then its hyperplanes are the 2-dimensional planes).  The hyperplane classifies all the training vectors in two classes. We can have many possible hyperplanes that are able to classify correctly all the elements in the feature set, but the best choice will be the hyperplane that leaves the Maximum Margin from both classes.</description>
    </item>
    
    <item>
      <title>Feature Selection using Boruta Algorithm</title>
      <link>/mlpost/feature-selection-using-boruta-algorithm/</link>
      <pubDate>Tue, 02 Apr 2019 00:00:00 +0000</pubDate>
      
      <guid>/mlpost/feature-selection-using-boruta-algorithm/</guid>
      <description>body {text-align: justify}Variable selection is an important aspect because it helps in building predictive models free from correlated variables, biases and unwanted noise.  The Boruta Algorithm is a feature selection algorithm. As a matter of interest, Boruta algorithm derive its name from a demon in Slavic mythology who lived in pine forests. 
How Boruta Algorithm works  Firstly, it adds randomness to the given data set by creating shuffled copies of all features which are called Shadow Features.</description>
    </item>
    
    <item>
      <title>Random Forest Hyperparameters Tuning</title>
      <link>/mlpost/random-forest-hyperparameters-tuning/</link>
      <pubDate>Tue, 02 Apr 2019 00:00:00 +0000</pubDate>
      
      <guid>/mlpost/random-forest-hyperparameters-tuning/</guid>
      <description>body {text-align: justify}Random Forest is a Bagging process of Ensemble Learners.  Random Forests are built from Decision Tree. Decision Trees work great, but they are not flexible when it comes to classify new samples. It creates a bootstrapped dataset with the same size of the original, and to do that Random Forest randomly selects rows with replacement. After creating a bootstrap dataset, it creates a decision tree using the bootstrapped dataset, but using only a subset of variables at each step.</description>
    </item>
    
    <item>
      <title>Principal Component Analysis</title>
      <link>/mlpost/principal-component-analysis/</link>
      <pubDate>Mon, 01 Apr 2019 00:00:00 +0000</pubDate>
      
      <guid>/mlpost/principal-component-analysis/</guid>
      <description>body {text-align: justify}Principal Component Analysis PCA is a deterministic method (given an input will always produce the same output).  It is always good to perform a PCA: Principal Components Analysis (PCA) is a data reduction technique that transforms a larger number of correlated variables into a much smaller set of uncorrelated variables called PRINCIPAL COMPONENTS. For example, we might use PCA to transform many correlated (and possibly redundant) variables into a less number of uncorrelated variables that retain as much information from the original set of variables.</description>
    </item>
    
    <item>
      <title>Polynomial Regression &amp; Smoothing Splines</title>
      <link>/mlpost/polynomial-regression-smoothing-splines/</link>
      <pubDate>Wed, 27 Mar 2019 00:00:00 +0000</pubDate>
      
      <guid>/mlpost/polynomial-regression-smoothing-splines/</guid>
      <description>body {text-align: justify}Polynomial Linear Regression Polynomial Linear Regression fits a nonlinear relationship between the value of x and the corresponding conditional mean of y, and has been used to describe nonlinear phenomena such as the progression of disease epidemics. Although polynomial regression fits a nonlinear model to the data, as a statistical estimation problem it is linear, in the sense that the regression function is linear in the unknown parameters that are estimated from the data.</description>
    </item>
    
    <item>
      <title>Fuzzy Matching Addresses to Prevent Fraudulent Application</title>
      <link>/mlpost/fuzzy-matching-addresses-to-prevent-fraudulent-application/</link>
      <pubDate>Mon, 25 Mar 2019 00:00:00 +0000</pubDate>
      
      <guid>/mlpost/fuzzy-matching-addresses-to-prevent-fraudulent-application/</guid>
      <description>body {text-align: justify}Application fraud refers to fraud committed by submitting a new credit application with fraudulent details to a credit provider. Normally, fraudsters collect the personal and financial data of innocent users from the identity documents, pay slips, bank statements, and other source documents to commit the application fraud. The information collected from all these documents will be either forged or sometimes the document itself will be stolen illegally or the details in the documents will be changed for the purpose of submitting a new credit application.</description>
    </item>
    
    <item>
      <title>Generalized Addictive Models GAMs</title>
      <link>/mlpost/generalized-addictive-models-gams/</link>
      <pubDate>Fri, 22 Mar 2019 00:00:00 +0000</pubDate>
      
      <guid>/mlpost/generalized-addictive-models-gams/</guid>
      <description>body {text-align: justify}Generalized Addictive Models GAMs incorporates non linear form of predictions, and are useful when we have not linearity between response variable and predictors. GAMs doesn’t force the predictors to a square as in polynomial regression, but GAMes tries to do a smooth line. The data we use here is biocapacity of different countries.
library(psych)eco &amp;lt;- read.csv(&amp;quot;C:/07 - R Website/dataset/ML/biocap.csv&amp;quot;)pairs.panels(eco, method = &amp;quot;pearson&amp;quot;, # correlation methodhist.</description>
    </item>
    
    <item>
      <title>Deal Multicollinearity with LASSO Regression</title>
      <link>/mlpost/deal-multicollinearity-with-lasso-regression/</link>
      <pubDate>Thu, 21 Mar 2019 00:00:00 +0000</pubDate>
      
      <guid>/mlpost/deal-multicollinearity-with-lasso-regression/</guid>
      <description>body {text-align: justify}Multicollinearity is a phenomenon in which two or more predictors in a multiple regression are highly correlated (R-squared more than 0.7), this can inflate our regression coefficients. We can test multicollinearity with the Variance Inflation Factor VIF is the ratio of variance in a model with multiple terms, divided by the variance of a model with one term alone. VIF = 1/1-R-squared. A rule of thumb is that if VIF &amp;gt; 10 then multicollinearity is high (a cutoff of 5 is also commonly used).</description>
    </item>
    
    <item>
      <title>Deal Multicollinearity with Ridge Regression</title>
      <link>/mlpost/deal-multicollinearity-with-ridge-regression/</link>
      <pubDate>Thu, 21 Mar 2019 00:00:00 +0000</pubDate>
      
      <guid>/mlpost/deal-multicollinearity-with-ridge-regression/</guid>
      <description>body {text-align: justify}Multicollinearity is a phenomenon in which two or more predictors in a multiple regression are highly correlated (R-squared more than 0.7), this can inflate our regression coefficients. We can test multicollinearity with the Variance Inflation Factor VIF is the ratio of variance in a model with multiple terms, divided by the variance of a model with one term alone. VIF = 1/1-R-squared. A rule of thumb is that if VIF &amp;gt; 10 then multicollinearity is high (a cutoff of 5 is also commonly used).</description>
    </item>
    
    <item>
      <title>Deal Outliers with Robust Regression</title>
      <link>/mlpost/deal-outliers-with-robust-regression/</link>
      <pubDate>Thu, 21 Mar 2019 00:00:00 +0000</pubDate>
      
      <guid>/mlpost/deal-outliers-with-robust-regression/</guid>
      <description>body {text-align: justify}This is a regression technique that can helps us alleviate the problem of outliers. Robust Regression is a family of regression techniques that is really quite immune to the presence of outliers. Least Trimmed Squares Regression is a technique that fit a regression function and is not effected by the presence of outliers. Least Trimmed Squares Regression attempts to minimise the sum of squared residuals over a subset of k points.</description>
    </item>
    
    <item>
      <title>Quantile Regression in Medical Expenditures</title>
      <link>/mlpost/quantile-regression-in-medical-expenditures/</link>
      <pubDate>Wed, 20 Mar 2019 00:00:00 +0000</pubDate>
      
      <guid>/mlpost/quantile-regression-in-medical-expenditures/</guid>
      <description>body {text-align: justify}The Quantile regression gives a more comprehensive picture of the effect of the independent variables on the dependent variable. Instead of estimating the model with average effects using the OLS linear model, the quantile regression produces different effects along the distribution (quantiles) of the dependent variable. The dependent variable is continuous with no zeros or too many repeated values.  Examples include estimating the effects of household income on food expenditures for low- and high-expenditure households, what are the factors influencing total medical expenditures for people with low, medium and high expenditures.</description>
    </item>
    
    <item>
      <title>Naive Bayes Classification</title>
      <link>/mlpost/naive-bayes-classification/</link>
      <pubDate>Thu, 14 Mar 2019 00:00:00 +0000</pubDate>
      
      <guid>/mlpost/naive-bayes-classification/</guid>
      <description>body {text-align: justify}Naive Bayes is an effective and commonly-used, machine learning classifier. It is a probabilistic classifier that makes classifications using the Maximum A Posteriori decision rule in a Bayesian setting. It can also be represented using a very simple Bayesian network. Naive Bayes classifiers have been especially popular for text classification, and are a traditional solution for problems such as spam detection.
An intuitive explanation for the Maximum A Posteriori Probability MAP is to think probabilities as degrees of belief.</description>
    </item>
    
    <item>
      <title>Extreme Gradient Boosting Algorithm</title>
      <link>/mlpost/extreme-gradient-boosting-algorithm/</link>
      <pubDate>Wed, 13 Mar 2019 00:00:00 +0000</pubDate>
      
      <guid>/mlpost/extreme-gradient-boosting-algorithm/</guid>
      <description>body {text-align: justify}Extreme Gradient Boosting is extensively used because is fast and accurate, and can handle missing values.  Gradient boosting is a machine learning technique for regression and classification problems, which produces a prediction model in the form of an ensemble of weak prediction models, typically decision trees. It builds the model in a stage-wise fashion like other boosting methods do, and it generalizes them by allowing optimization of an arbitrary differentiable loss function.</description>
    </item>
    
    <item>
      <title>Customer segmentation via K-Means &amp; Hierarchical clustering</title>
      <link>/mlpost/customer-segmentation/</link>
      <pubDate>Sat, 12 Jan 2019 00:00:00 +0000</pubDate>
      
      <guid>/mlpost/customer-segmentation/</guid>
      <description>body {text-align: justify}Consider to have a big mall in a specific city that contains information of its clients that subcribed to a membership card. The last feature is Spending Score that is a score that the mall computed for each of their clients based on several criteria including for example their income and the number of times per week they show up in the mall and of course, the amount of dollars they spent in a year.</description>
    </item>
    
    <item>
      <title>Assessing the sucess of a new product via multiple classifiers</title>
      <link>/mlpost/estimate-the-sucess-of-a-new-product-with-logistic-regression/</link>
      <pubDate>Thu, 10 Jan 2019 00:00:00 +0000</pubDate>
      
      <guid>/mlpost/estimate-the-sucess-of-a-new-product-with-logistic-regression/</guid>
      <description>body {text-align: justify}These are a series of analysis to illustate the main classification algorithms and their advantages.
The table shows the business clients of a company that has just launched a new product online. Some of the clients responded positively to the ads by buying the product and other responded negatively by not buying the product. The last column of the table tells for each user if the user bought the product or not.</description>
    </item>
    
  </channel>
</rss>