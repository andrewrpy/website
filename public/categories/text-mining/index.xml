<?xml version="1.0" encoding="utf-8" standalone="yes" ?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title>text mining on Andrea Perlato</title>
    <link>/categories/text-mining/</link>
    <description>Recent content in text mining on Andrea Perlato</description>
    <generator>Hugo -- gohugo.io</generator>
    <language>en-us</language>
    <lastBuildDate>Thu, 14 Nov 2019 00:00:00 +0000</lastBuildDate>
    
	<atom:link href="/categories/text-mining/index.xml" rel="self" type="application/rss+xml" />
    
    
    <item>
      <title>NLP Step by Step</title>
      <link>/mlpost/nlp-step-by-step/</link>
      <pubDate>Thu, 14 Nov 2019 00:00:00 +0000</pubDate>
      
      <guid>/mlpost/nlp-step-by-step/</guid>
      <description>body {text-align: justify}This post has the aim to shows all the processes related to the NLP and how to use the Naive Bayes Classifier using Python and the nltk library.We use data from spam detection.
import pandas as pdimport numpy as npimport nltk#import matplotlib.pyplot as plt#import seaborn as snsmessages = pd.read_csv(&amp;#39;C:/07 - R Website/dataset/PY/SMSSpamCollection&amp;#39;, sep=&amp;#39;\t&amp;#39;, names=[&amp;#39;label&amp;#39;,&amp;#39;message&amp;#39;])messages.head()## label message## 0 ham Go until jurong point, crazy.</description>
    </item>
    
    <item>
      <title>Extract the Main Topics from Books</title>
      <link>/mlpost/extract-the-main-topics-from-books/</link>
      <pubDate>Fri, 09 Aug 2019 00:00:00 +0000</pubDate>
      
      <guid>/mlpost/extract-the-main-topics-from-books/</guid>
      <description>body {text-align: justify}Topic modeling is the process of identifying topics in a set of documents. This can be useful for search engines, customer service automation, and any other instance where knowing the topics of documents is important. There are multiple methods of going about doing this. The most commonly used is Latent Dirichlet Allocation.The LDA builds a topic per document model and words per topic model, modeled as Dirichlet distributions.</description>
    </item>
    
    <item>
      <title>Introduction to Topic Model</title>
      <link>/theorypost/introduction-to-topic-model/</link>
      <pubDate>Thu, 08 Aug 2019 00:00:00 +0000</pubDate>
      
      <guid>/theorypost/introduction-to-topic-model/</guid>
      <description>body {text-align: justify}The Topic Model is a type of statistical model to find the topics that occur in a collection of documents.It is a frequently used text-mining tool for discovery of hidden semantic structures in a text body. For example, image to have some articles or a series of social media messages and we want to understand what is going on inside of them. A common tool to face this problem is via Unsupervised Machine Learning model.</description>
    </item>
    
    <item>
      <title>Introduction to Naive Bayes</title>
      <link>/theorypost/introduction-to-naive-bayes/</link>
      <pubDate>Tue, 06 Aug 2019 00:00:00 +0000</pubDate>
      
      <guid>/theorypost/introduction-to-naive-bayes/</guid>
      <description>body {text-align: justify}It is a Probability Classifier. Naïve Bayes is the first algorithm that should be considered for solving Text Classification Problem which involves High Dimensional training Dataset. A few examples are: Sentiment Analysis and Classifying Topics on Social Media. It also refers to the Bayes’ Theorem also known as Bayes’ Law that give us a method to calculate the Conditional Probability: that is the probability of an event, based on previous knowledge available on the events.</description>
    </item>
    
    <item>
      <title>Introduction to Word2Vec</title>
      <link>/theorypost/introduction-to-word2vec/</link>
      <pubDate>Tue, 06 Aug 2019 00:00:00 +0000</pubDate>
      
      <guid>/theorypost/introduction-to-word2vec/</guid>
      <description>body {text-align: justify}The Word2Vec is a deep learning algorithm that draws context from phrases. Every textual document is represented in the form of a vector, and that is done through Vector Space Modelling (VSM). We can convert our text using One Hot Encoding. For example, having three words we can make a vector in a three dimentional space.
The problem with One Hot Encoding is that it doesn’t help us to find similarities.</description>
    </item>
    
    <item>
      <title>Predict Movie Sentiment via DOC2VEC</title>
      <link>/aipost/predict-movie-sentiment-via-doc2vec/</link>
      <pubDate>Tue, 06 Aug 2019 00:00:00 +0000</pubDate>
      
      <guid>/aipost/predict-movie-sentiment-via-doc2vec/</guid>
      <description>body {text-align: justify}In order to have an introduction of the Word2Vec look at this post.Using this method we try to predict the movie sentiment (positive vs. negative) using text data as predictor. We use the movie review data set, and we use the power of Doc2Vec to transform our data into predictors.
library(text2vec)library(tm)data(movie_review)names(movie_review)[1] &amp;quot;id&amp;quot; &amp;quot;sentiment&amp;quot; &amp;quot;review&amp;quot; The data set contain an id, sentiment (0=negative, 1=positive), and review that contains the text if people like the movie or not.</description>
    </item>
    
    <item>
      <title>Identify Spam Emails</title>
      <link>/mlpost/identify-smap-emails/</link>
      <pubDate>Fri, 02 Aug 2019 00:00:00 +0000</pubDate>
      
      <guid>/mlpost/identify-smap-emails/</guid>
      <description>body {text-align: justify}We want to differentiate between spam (called spam) and non-spam (called ham) email based on the content. We use a training set of textual data that are already labeled spam/non-spam email.
We start removing empy columns, and we call our columns label and text.We also create a corpus, remove punctuation, transform everything into lowercase, remove numbers, and stop words. Then, we have to stamming the document, and finally we have a corpus of terms.</description>
    </item>
    
    <item>
      <title>Text Mining Clustering Tweets</title>
      <link>/mlpost/text-mining-clustering-tweets/</link>
      <pubDate>Fri, 02 Aug 2019 00:00:00 +0000</pubDate>
      
      <guid>/mlpost/text-mining-clustering-tweets/</guid>
      <description>body {text-align: justify}We can use Unsupervised Classification (clustering) to learn more about text that we have to analyze. More specifically, we use Hierarchical clustering to cluster our text into groups that are the propensity to occur together. In this example, text are some tweet about Catalan Independence Referendum.
First step is to convert our tweets into a corpus, remove punctuation, transform everything into lowercase, remove numbers, and stop words.</description>
    </item>
    
  </channel>
</rss>