<?xml version="1.0" encoding="utf-8" standalone="yes" ?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title>text mining on Andrea Perlato</title>
    <link>/categories/text-mining/</link>
    <description>Recent content in text mining on Andrea Perlato</description>
    <generator>Hugo -- gohugo.io</generator>
    <language>en-us</language>
    <lastBuildDate>Tue, 06 Aug 2019 00:00:00 +0000</lastBuildDate>
    
	<atom:link href="/categories/text-mining/index.xml" rel="self" type="application/rss+xml" />
    
    
    <item>
      <title>Introduction to Naive Bayes</title>
      <link>/theorypost/introduction-to-naive-bayes/</link>
      <pubDate>Tue, 06 Aug 2019 00:00:00 +0000</pubDate>
      
      <guid>/theorypost/introduction-to-naive-bayes/</guid>
      <description>body {text-align: justify}It is a Probability Classifier. Naïve Bayes is the first algorithm that should be considered for solving Text Classification Problem* which involves High Dimensional training Dataset. A few examples are: Sentiment Analysis and Classifying Topics on Social Media. It also refers to the Bayes’ Theorem also known as Bayes’ Law that give us a method to calculate the Conditional Probability: that is the probability of an event, based on previous knowledge available on the events.</description>
    </item>
    
    <item>
      <title>Introduction to Word2Vec</title>
      <link>/theorypost/introduction-to-word2vec/</link>
      <pubDate>Tue, 06 Aug 2019 00:00:00 +0000</pubDate>
      
      <guid>/theorypost/introduction-to-word2vec/</guid>
      <description>body {text-align: justify}The Word2Vec is a deep learning algorithm that draws context from phrases. Every textual document is represented in the form of a vector, and that is done through Vector Space Modelling (VSM). We can convert our text using One Hot Encoding. For example, having three words we can make a vector in a three dimentional space.
The problem with One Hot Encoding is that it doesn’t help us to find similarities.</description>
    </item>
    
    <item>
      <title>Predict Movie Sentiment via DOC2VEC</title>
      <link>/aipost/predict-movie-sentiment-via-doc2vec/</link>
      <pubDate>Tue, 06 Aug 2019 00:00:00 +0000</pubDate>
      
      <guid>/aipost/predict-movie-sentiment-via-doc2vec/</guid>
      <description>body {text-align: justify}In order to have an introduction of the Word2Vec look at this post.</description>
    </item>
    
    <item>
      <title>Identify Spam Emails</title>
      <link>/mlpost/identify-smap-emails/</link>
      <pubDate>Fri, 02 Aug 2019 00:00:00 +0000</pubDate>
      
      <guid>/mlpost/identify-smap-emails/</guid>
      <description>body {text-align: justify}We want to differentiate between spam (called spam) and non-spam (called ham) email based on the content. We use a training set of textual data that are already labeled spam/non-spam email.
We start removing empy columns, and we call our columns label and text.We also create a corpus, remove punctuation, transform everything into lowercase, remove numbers, and stop words. Then, we have to stamming the document, and finally we have a corpus of terms.</description>
    </item>
    
    <item>
      <title>Text Mining Clustering Tweets</title>
      <link>/mlpost/text-mining-clustering-tweets/</link>
      <pubDate>Fri, 02 Aug 2019 00:00:00 +0000</pubDate>
      
      <guid>/mlpost/text-mining-clustering-tweets/</guid>
      <description>body {text-align: justify}We can use Unsupervised Classification (clustering) to learn more about text that we have to analyze. More specifically, we use Hierarchical clustering to cluster our text into groups that are the propensity to occur together. In this example, text are some tweet about Catalan Independence Referendum.
First step is to convert our tweets into a corpus, remove punctuation, transform everything into lowercase, remove numbers, and stop words.</description>
    </item>
    
  </channel>
</rss>