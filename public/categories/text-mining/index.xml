<?xml version="1.0" encoding="utf-8" standalone="yes" ?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title>text mining on Andrea Perlato</title>
    <link>/categories/text-mining/</link>
    <description>Recent content in text mining on Andrea Perlato</description>
    <generator>Hugo -- gohugo.io</generator>
    <language>en-us</language>
    <lastBuildDate>Fri, 03 Jan 2020 00:00:00 +0000</lastBuildDate>
    
	<atom:link href="/categories/text-mining/index.xml" rel="self" type="application/rss+xml" />
    
    
    <item>
      <title>Detect Scientific Topics using NLP</title>
      <link>/mlpost/detect-scientific-topics-using-nlp/</link>
      <pubDate>Fri, 03 Jan 2020 00:00:00 +0000</pubDate>
      
      <guid>/mlpost/detect-scientific-topics-using-nlp/</guid>
      <description>body {text-align: justify}Studying related work within scientific literature is a crucial step in order to have a global overview about the scientific topics, but the amount of digital text data is growing exponentially and it is time consuming for researchers to find relevant information. Making scientific recommender systems smarter is crucial in order to help scientists in their bibliographical research phase. 
How to tackle this problem to extract the topics: articles vs.</description>
    </item>
    
    <item>
      <title>NLP Step by Step</title>
      <link>/mlpost/nlp-step-by-step/</link>
      <pubDate>Thu, 14 Nov 2019 00:00:00 +0000</pubDate>
      
      <guid>/mlpost/nlp-step-by-step/</guid>
      <description>body {text-align: justify}This post has the aim to shows all the processes related to the NLP and how to use the Naive Bayes Classifier using Python and the nltk library.We use data from spam detection.
In NLP a large part of the processing is Feature Engeneering. Tipically the steps are: Regular Expression: that is a formal language for specifying text strings: for example, we can have for the same word the s for plural, the capital first letter and any combination of those.</description>
    </item>
    
    <item>
      <title>Extract the Main Topics from Books</title>
      <link>/mlpost/extract-the-main-topics-from-books/</link>
      <pubDate>Fri, 09 Aug 2019 00:00:00 +0000</pubDate>
      
      <guid>/mlpost/extract-the-main-topics-from-books/</guid>
      <description>body {text-align: justify}Topic modeling is the process of identifying topics in a set of documents. This can be useful for search engines, customer service automation, and any other instance where knowing the topics of documents is important. There are multiple methods of going about doing this. The most commonly used is Latent Dirichlet Allocation.The LDA builds a topic per document model and words per topic model, modeled as Dirichlet distributions.</description>
    </item>
    
    <item>
      <title>Introduction to Topic Model</title>
      <link>/theorypost/introduction-to-topic-model/</link>
      <pubDate>Thu, 08 Aug 2019 00:00:00 +0000</pubDate>
      
      <guid>/theorypost/introduction-to-topic-model/</guid>
      <description>body {text-align: justify}The Topic Model is a type of statistical model to find the topics that occur in a collection of documents.It is a frequently used text-mining tool for discovery of hidden semantic structures in a text body. For example, image to have some articles or a series of social media messages and we want to understand what is going on inside of them. A common tool to face this problem is via Unsupervised Machine Learning model.</description>
    </item>
    
    <item>
      <title>Introduction to Naive Bayes</title>
      <link>/theorypost/introduction-to-naive-bayes/</link>
      <pubDate>Tue, 06 Aug 2019 00:00:00 +0000</pubDate>
      
      <guid>/theorypost/introduction-to-naive-bayes/</guid>
      <description>body {text-align: justify}It is a Probability Classifier. Naïve Bayes is the first algorithm that should be considered for solving Text Classification Problem which involves High Dimensional training Dataset. A few examples are: Sentiment Analysis and Classifying Topics on Social Media. It also refers to the Bayes’ Theorem also known as Bayes’ Law that give us a method to calculate the Conditional Probability: that is the probability of an event, based on previous knowledge available on the events.</description>
    </item>
    
    <item>
      <title>Introduction to Word2Vec</title>
      <link>/theorypost/introduction-to-word2vec/</link>
      <pubDate>Tue, 06 Aug 2019 00:00:00 +0000</pubDate>
      
      <guid>/theorypost/introduction-to-word2vec/</guid>
      <description>body {text-align: justify}The Word2Vec is a semantic learning framework that used a neural network to learn the representation of words or phrases in a text. It is usefull to understand the semantic meaning behind a term. This algorithm use two methods: 1 - CBOW 2 - SkipGram In Continuous bag of words CBOW predicts the current word from a window of surrounding context words, or given a set of context words predicts the missing word that is likely to appear in that context.</description>
    </item>
    
    <item>
      <title>Predict Movie Sentiment via DOC2VEC</title>
      <link>/aipost/predict-movie-sentiment-via-doc2vec/</link>
      <pubDate>Tue, 06 Aug 2019 00:00:00 +0000</pubDate>
      
      <guid>/aipost/predict-movie-sentiment-via-doc2vec/</guid>
      <description>body {text-align: justify}In order to have an introduction of the Word2Vec look at this post.Using this method we try to predict the movie sentiment (positive vs. negative) using text data as predictor. We use the movie review data set, and we use the power of Doc2Vec to transform our data into predictors.
library(text2vec)library(tm)data(movie_review)names(movie_review)[1] &amp;quot;id&amp;quot; &amp;quot;sentiment&amp;quot; &amp;quot;review&amp;quot; The data set contain an id, sentiment (0=negative, 1=positive), and review that contains the text if people like the movie or not.</description>
    </item>
    
    <item>
      <title>Identify Spam Emails</title>
      <link>/mlpost/identify-smap-emails/</link>
      <pubDate>Fri, 02 Aug 2019 00:00:00 +0000</pubDate>
      
      <guid>/mlpost/identify-smap-emails/</guid>
      <description>body {text-align: justify}We want to differentiate between spam (called spam) and non-spam (called ham) email based on the content. We use a training set of textual data that are already labeled spam/non-spam email.
We start removing empy columns, and we call our columns label and text.We also create a corpus, remove punctuation, transform everything into lowercase, remove numbers, and stop words. Then, we have to stamming the document, and finally we have a corpus of terms.</description>
    </item>
    
    <item>
      <title>Text Mining Clustering Tweets</title>
      <link>/mlpost/text-mining-clustering-tweets/</link>
      <pubDate>Fri, 02 Aug 2019 00:00:00 +0000</pubDate>
      
      <guid>/mlpost/text-mining-clustering-tweets/</guid>
      <description>body {text-align: justify}We can use Unsupervised Classification (clustering) to learn more about text that we have to analyze. More specifically, we use Hierarchical clustering to cluster our text into groups that are the propensity to occur together. In this example, text are some tweet about Catalan Independence Referendum.
First step is to convert our tweets into a corpus, remove punctuation, transform everything into lowercase, remove numbers, and stop words.</description>
    </item>
    
  </channel>
</rss>