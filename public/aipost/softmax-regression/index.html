<!DOCTYPE html>
<html lang="en-us">
  <head>
    <meta charset="utf-8">
<meta name="viewport" content="width=device-width, initial-scale=1.0">
<meta name="generator" content="Hugo 0.55.6" />


<title>Softmax Regression - Andrea Perlato</title>
<meta property="og:title" content="Softmax Regression - Andrea Perlato">



  







<link rel="stylesheet" href="/css/fonts.css" media="all">
<link rel="stylesheet" href="/css/main.css" media="all">



  </head>
  <body>
    <div class="wrapper">
      <header class="header">
        <nav class="nav">
  <a href="/" class="nav-logo">
    <img src="/images/logo.png"
         width="50"
         height="50"
         alt="Logo">
  </a>

  <ul class="nav-links">
    
    <li><a href="/about/">About</a></li>
    
    <li><a href="/graphpost/">Graph</a></li>
    
    <li><a href="/mlpost/">Machine Learning</a></li>
    
    <li><a href="/aipost/">Artificial Intelligence</a></li>
    
    <li><a href="/tspost/">Time Series</a></li>
    
    <li><a href="/theorypost/">Theory</a></li>
    
  </ul>
</nav>

      </header>


<main class="content" role="main">

  <article class="article">
    

    <h1 class="article-title">Softmax Regression</h1>

    

    <div class="article-content">
      


<style>
body {
text-align: justify}
</style>
<p>When we have to deal with a classification with more than 2 possible levels, we use a generalization of the logistic regression function called <a href="https://en.wikipedia.org/wiki/Softmax_function"><strong>softmax regression</strong></a>; a logistic regression class for multi-class classification tasks. In Softmax Regression, we replace the sigmoid logistic function by the so-called softmax function.</p>
<p><span class="math display">\[
\begin{array}{l}{\qquad P\left(y=j | z^{(i)}\right)=\phi_{\text {softmax}}\left(z^{(i)}\right)=\frac{e^{z^{(i)}}}{\sum_{j=0}^{k} e^{z_{k}^{(i)}}}} \\ {\text { where we define the net input } z \text { as }} \\ {\qquad z=w_{1} x_{1}+\ldots+w_{m} x_{m}+b=\sum_{l=1}^{m} w_{l} x_{l}+b=\mathbf{w}^{T} \mathbf{x}+b}\end{array}
\]</span></p>
<p>The <strong>w</strong> is the weight vector, <strong>x</strong> is the feature vector of 1 training sample, and <strong>b</strong> is the bias unit. </br>
A <strong>bias unit</strong> is an <strong>extra neuron</strong> added to each pre-output layer that stores the value of 1. Bias units aren’t connected to any previous layer and in this sense don’t represent a true activity. It is used in the case the sum of the weights is equal to zero. </br>
Now, this softmax function computes the probability that this training sample x(i) belongs to class j given the weight and net input z(i). So, we compute the probability <strong>p(y=j∣x(i);wj</strong>) for each class label in j=1,…,k.. Note the normalization term in the denominator which causes these class probabilities to sum up to one.</p>
<p>Even if it is a bit unusual, we use the softmax regression as the activation function of the <strong>output layer y</strong>.</p>
<p><span class="math display">\[
\begin{array}{l}{t=e^{\left(z^{(1)}\right)}} \\ {a^{(L)}=\frac{e^{z^{(L)}}}{\sum_{j={1}}^{4} t_{i}}, \quad a_{i}^{(L)}=\frac{t_{i}}{\sum_{j={1}}^{\frac{4} t_{i}}}}\end{array}
\]</span>
The formula above assumes that we have 4 layers L on the output y as depicted in the figure below.</p>
<center>
<img src="/img/softmaxregression.png" style="width:80.0%" />
</center>
<p>In the figure above we have z^(L) set to (5, 2, -1, 3), and from these values we can compute the activation function t suing the formula represented in the calculation we made before. Having the value of <strong>t</strong>, we can calculate the activation function <strong>a</strong> for the output <strong>y</strong>.
From the example above, the output with <strong>level_0=0.842</strong> is the most likely to categorize what we received in input. </br>
The categorization is called <strong>hard max</strong> and gives <strong>1</strong> to level_0 and 0 to the other output values. On the contray, the value <strong>0.842</strong> is called <strong>soft max</strong>.</p>
<p>The loss function L is calculated as follow:
<span class="math display">\[
f(\hat{y}, y)=-\sum_{j=1}^{4} y_{j} \log \hat{y}_{j}
\]</span></p>
<p>For the values categorized with 0 in hard max, the yi is zero and so the loss finction is just equal to <strong>-log(yi)</strong>. More generally, what the loss function does is to looks, at whatever is the ground true in our training-set, at the correspoding probability of that class and put it as high as possible. This is quite similar to the <a href="https://en.wikipedia.org/wiki/Maximum_likelihood_estimation"><strong>maximum likehood estimation</strong></a>.</p>
<p>Reference: <a href="https://www.coursera.org/learn/deep-neural-network/home/welcome"><strong>coursera deep neural network course</strong></a></p>

    </div>
  </article>

  


</main>

      <footer class="footer">
        <ul class="footer-links">
          <li>
            <a href="/index.xml" type="application/rss+xml" target="_blank">RSS feed</a>
          </li>
          <li>
            <a href="https://www.rstudio.com/" class="footer-links-kudos">Made with <img src="/images/hugo-logo.png" alt="Img link to Hugo website" width="22" height="22"></a>
          </li>
        </ul>
      </footer>

    </div>
    

    
<script src="/js/math-code.js"></script>
<script async src="//cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.5/MathJax.js?config=TeX-MML-AM_CHTML"></script>


    
<script type="application/javascript">
var doNotTrack = false;
if (!doNotTrack) {
	(function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
	(i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
	m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
	})(window,document,'script','https://www.google-analytics.com/analytics.js','ga');
	ga('create', 'UA-51254710-89', 'auto');
	
	ga('send', 'pageview');
}
</script>

  </body>
</html>

