<!DOCTYPE html>
<html lang="en-us">
  <head>
    <meta charset="utf-8">
<meta name="viewport" content="width=device-width, initial-scale=1.0">
<meta name="generator" content="Hugo 0.55.6" />


<title>Gradient Checking - Andrea Perlato</title>
<meta property="og:title" content="Gradient Checking - Andrea Perlato">



  







<link rel="stylesheet" href="/css/fonts.css" media="all">
<link rel="stylesheet" href="/css/main.css" media="all">



  </head>
  <body>
    <div class="wrapper">
      <header class="header">
        <nav class="nav">
  <a href="/" class="nav-logo">
    <img src="/images/logo.png"
         width="50"
         height="50"
         alt="Logo">
  </a>

  <ul class="nav-links">
    
    <li><a href="/about/">About</a></li>
    
    <li><a href="/graphpost/">Graph</a></li>
    
    <li><a href="/mlpost/">Machine Learning</a></li>
    
    <li><a href="/aipost/">Artificial Intelligence</a></li>
    
    <li><a href="/tspost/">Time Series</a></li>
    
    <li><a href="/theorypost/">Theory</a></li>
    
  </ul>
</nav>

      </header>


<main class="content" role="main">

  <article class="article">
    

    <h1 class="article-title">Gradient Checking</h1>

    

    <div class="article-content">
      


<style>
body {
text-align: justify}
</style>
<p>When we implement <a href="https://www.andreaperlato.com/aipost/backpropagation/"><strong>backpropagation</strong></a> there is a test called <strong>Gradient Checking</strong> that helps to make sure that the implementation of backpropagation is correct.</p>
<center>
<img src="/img/checkgradientfunc.png" style="width:40.0%" />
</center>
<p>Looking at the figure above we can get much better estimate of gradient if we use a larger approximation of the derivative using a double triangle.
The hiight of the triagle in the figure can be seen as follow:</p>
<p><span class="math display">\[
\begin{array}{l}{\frac{f(\theta+\epsilon)-f(\theta-\epsilon)}{2 \varepsilon} \approx g(\theta)} \\ {\frac{(1.01)^{3}-(0.99)^{3}}{2(0.01)}=3.0001} \\ {g(\theta)=3 \theta^{2}=3}\end{array}
\]</span>
From the calculation above, we havean approximation error of 0.0001 extremely colose to 3. </br>
This method is called <strong>Two Side Approxicamtion</strong> of epsilon. </br>
It is important to remeber that: </br>
1 - this method is not used during the training of the neural network but only to debug </br>
2 - when we have the theta function very far from the real function, remember to use regularization </br>
3 - it doesnâ€™t work with dropout, because in this case the cost function J is very difficult to compute </br>
4 - run at random initialization: it is not impossible that the implementation of gradient descent is correct when weights are close to zero.</p>
<p>Reference: <a href="https://www.coursera.org/learn/deep-neural-network/home/welcome"><strong>coursera deep neural network course</strong></a></p>

    </div>
  </article>

  


</main>

      <footer class="footer">
        <ul class="footer-links">
          <li>
            <a href="/index.xml" type="application/rss+xml" target="_blank">RSS feed</a>
          </li>
          <li>
            <a href="https://www.rstudio.com/" class="footer-links-kudos">Made with <img src="/images/hugo-logo.png" alt="Img link to Hugo website" width="22" height="22"></a>
          </li>
        </ul>
      </footer>

    </div>
    

    
<script src="/js/math-code.js"></script>
<script async src="//cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.5/MathJax.js?config=TeX-MML-AM_CHTML"></script>


    
<script type="application/javascript">
var doNotTrack = false;
if (!doNotTrack) {
	(function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
	(i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
	m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
	})(window,document,'script','https://www.google-analytics.com/analytics.js','ga');
	ga('create', 'UA-51254710-89', 'auto');
	
	ga('send', 'pageview');
}
</script>

  </body>
</html>

