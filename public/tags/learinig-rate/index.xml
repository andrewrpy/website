<?xml version="1.0" encoding="utf-8" standalone="yes" ?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title>learinig rate on Andrea Perlato</title>
    <link>/tags/learinig-rate/</link>
    <description>Recent content in learinig rate on Andrea Perlato</description>
    <generator>Hugo -- gohugo.io</generator>
    <language>en-us</language>
    <lastBuildDate>Tue, 17 Dec 2019 00:00:00 +0000</lastBuildDate>
    
	<atom:link href="/tags/learinig-rate/index.xml" rel="self" type="application/rss+xml" />
    
    
    <item>
      <title>The Learning Rate</title>
      <link>/theorypost/the-learning-rate/</link>
      <pubDate>Tue, 17 Dec 2019 00:00:00 +0000</pubDate>
      
      <guid>/theorypost/the-learning-rate/</guid>
      <description>body {text-align: justify}Deep learning neural networks are trained using the stochastic gradient descent algorithm.Stochastic gradient descent is an optimization algorithm that estimates the error gradient for the current state of the model using examples from the training dataset, then updates the weights of the model using the backpropagation of errors algorithm.
The amount that the weights are updated during training is referred to as the step size or the learning rate.</description>
    </item>
    
  </channel>
</rss>