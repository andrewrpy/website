<?xml version="1.0" encoding="utf-8" standalone="yes" ?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title>l2 regularization on Andrea Perlato</title>
    <link>/tags/l2-regularization/</link>
    <description>Recent content in l2 regularization on Andrea Perlato</description>
    <generator>Hugo -- gohugo.io</generator>
    <language>en-us</language>
    <lastBuildDate>Tue, 18 Jun 2019 00:00:00 +0000</lastBuildDate>
    
	<atom:link href="/tags/l2-regularization/index.xml" rel="self" type="application/rss+xml" />
    
    
    <item>
      <title>Ridge and Lasso Regression</title>
      <link>/theorypost/ridge-and-lasso-regression/</link>
      <pubDate>Tue, 18 Jun 2019 00:00:00 +0000</pubDate>
      
      <guid>/theorypost/ridge-and-lasso-regression/</guid>
      <description>body {text-align: justify}This article is a summary of the StatQuest video made by Josh Starmer. Click here to see the video on Ridge Regression explained by Josh Starmer. Click here to see the video on Lasso Regression explained by Josh Starmer. 
Overfitting In statistics, overfitting is the production of an analysis that corresponds too closely or exactly to a particular set of data, and may therefore fail to fit additional data or predict future observations reliably.</description>
    </item>
    
  </channel>
</rss>