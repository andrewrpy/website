<?xml version="1.0" encoding="utf-8" standalone="yes" ?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title>backpropagation on Andrea Perlato</title>
    <link>/tags/backpropagation/</link>
    <description>Recent content in backpropagation on Andrea Perlato</description>
    <generator>Hugo -- gohugo.io</generator>
    <language>en-us</language>
    <lastBuildDate>Fri, 22 Nov 2019 00:00:00 +0000</lastBuildDate>
    
	<atom:link href="/tags/backpropagation/index.xml" rel="self" type="application/rss+xml" />
    
    
    <item>
      <title>Gradient Checking</title>
      <link>/aipost/gradient-checking/</link>
      <pubDate>Fri, 22 Nov 2019 00:00:00 +0000</pubDate>
      
      <guid>/aipost/gradient-checking/</guid>
      <description>body {text-align: justify}When we implement backpropagation there is a test called Gradient Checking that helps to make sure that the implementation of backpropagation is correct.</description>
    </item>
    
    <item>
      <title>Backpropagation Intuition</title>
      <link>/aipost/backpropagation/</link>
      <pubDate>Mon, 17 Jun 2019 00:00:00 +0000</pubDate>
      
      <guid>/aipost/backpropagation/</guid>
      <description>body {text-align: justify}Introduction  We already know that there is in ANN a Forwward Propagation where the information is entered into the input layer, and then it is propagated forward to get our output values to compare with the actual values that we have in our training set, and then we calculate the errors. Then the errors are back propagated through the network in the opposite direction in order to adjust the weights.</description>
    </item>
    
  </channel>
</rss>