<?xml version="1.0" encoding="utf-8" standalone="yes" ?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title>TheoryPosts on Andrea Perlato</title>
    <link>/theorypost/</link>
    <description>Recent content in TheoryPosts on Andrea Perlato</description>
    <generator>Hugo -- gohugo.io</generator>
    <language>en-us</language>
    <lastBuildDate>Sat, 04 Apr 2020 00:00:00 +0000</lastBuildDate>
    
	<atom:link href="/theorypost/index.xml" rel="self" type="application/rss+xml" />
    
    
    <item>
      <title>Cloud Services Video Interview</title>
      <link>/theorypost/cloud-servises-video-interview/</link>
      <pubDate>Sat, 04 Apr 2020 00:00:00 +0000</pubDate>
      
      <guid>/theorypost/cloud-servises-video-interview/</guid>
      <description>body {text-align: justify}.rwd-video {height: 0;overflow: hidden;padding-bottom: 56.25%;padding-top: 30px;position: relative;}.rwd-video iframe,.rwd-video object,.rwd-video embed {height: 100%;left: 0;position: absolute;top: 0;width: 100%;}</description>
    </item>
    
    <item>
      <title>Feature Engineering Video Interview</title>
      <link>/theorypost/feature-engineering-video-interview/</link>
      <pubDate>Sat, 04 Apr 2020 00:00:00 +0000</pubDate>
      
      <guid>/theorypost/feature-engineering-video-interview/</guid>
      <description>body {text-align: justify}.rwd-video {height: 0;overflow: hidden;padding-bottom: 56.25%;padding-top: 30px;position: relative;}.rwd-video iframe,.rwd-video object,.rwd-video embed {height: 100%;left: 0;position: absolute;top: 0;width: 100%;}</description>
    </item>
    
    <item>
      <title>Feature Selection Video Interview</title>
      <link>/theorypost/feature-selection-video-interview/</link>
      <pubDate>Sat, 04 Apr 2020 00:00:00 +0000</pubDate>
      
      <guid>/theorypost/feature-selection-video-interview/</guid>
      <description>body {text-align: justify}.rwd-video {height: 0;overflow: hidden;padding-bottom: 56.25%;padding-top: 30px;position: relative;}.rwd-video iframe,.rwd-video object,.rwd-video embed {height: 100%;left: 0;position: absolute;top: 0;width: 100%;}</description>
    </item>
    
    <item>
      <title>Machine Learning Algorithms Video Interview</title>
      <link>/theorypost/machine-learning-algorithms-video-interview/</link>
      <pubDate>Sat, 04 Apr 2020 00:00:00 +0000</pubDate>
      
      <guid>/theorypost/machine-learning-algorithms-video-interview/</guid>
      <description>body {text-align: justify}.rwd-video {height: 0;overflow: hidden;padding-bottom: 56.25%;padding-top: 30px;position: relative;}.rwd-video iframe,.rwd-video object,.rwd-video embed {height: 100%;left: 0;position: absolute;top: 0;width: 100%;}</description>
    </item>
    
    <item>
      <title>Machine Learning Pipeline Video Interview</title>
      <link>/theorypost/machine-learning-pipeline-video-interview/</link>
      <pubDate>Sat, 04 Apr 2020 00:00:00 +0000</pubDate>
      
      <guid>/theorypost/machine-learning-pipeline-video-interview/</guid>
      <description>body {text-align: justify}.rwd-video {height: 0;overflow: hidden;padding-bottom: 56.25%;padding-top: 30px;position: relative;}.rwd-video iframe,.rwd-video object,.rwd-video embed {height: 100%;left: 0;position: absolute;top: 0;width: 100%;}</description>
    </item>
    
    <item>
      <title>Model Deployment Video Interview</title>
      <link>/theorypost/model-deployment-video-interview/</link>
      <pubDate>Sat, 04 Apr 2020 00:00:00 +0000</pubDate>
      
      <guid>/theorypost/model-deployment-video-interview/</guid>
      <description>body {text-align: justify}.rwd-video {height: 0;overflow: hidden;padding-bottom: 56.25%;padding-top: 30px;position: relative;}.rwd-video iframe,.rwd-video object,.rwd-video embed {height: 100%;left: 0;position: absolute;top: 0;width: 100%;}</description>
    </item>
    
    <item>
      <title>Model Performance Video Interview</title>
      <link>/theorypost/model-performance-video-interview/</link>
      <pubDate>Sat, 04 Apr 2020 00:00:00 +0000</pubDate>
      
      <guid>/theorypost/model-performance-video-interview/</guid>
      <description>body {text-align: justify}.rwd-video {height: 0;overflow: hidden;padding-bottom: 56.25%;padding-top: 30px;position: relative;}.rwd-video iframe,.rwd-video object,.rwd-video embed {height: 100%;left: 0;position: absolute;top: 0;width: 100%;}</description>
    </item>
    
    <item>
      <title>Natural Language Processing Video Interview</title>
      <link>/theorypost/natural-language-processing-video-interview/</link>
      <pubDate>Sat, 04 Apr 2020 00:00:00 +0000</pubDate>
      
      <guid>/theorypost/natural-language-processing-video-interview/</guid>
      <description>body {text-align: justify}.rwd-video {height: 0;overflow: hidden;padding-bottom: 56.25%;padding-top: 30px;position: relative;}.rwd-video iframe,.rwd-video object,.rwd-video embed {height: 100%;left: 0;position: absolute;top: 0;width: 100%;}</description>
    </item>
    
    <item>
      <title>R and Python Video Interview</title>
      <link>/theorypost/r-and-python/</link>
      <pubDate>Sat, 04 Apr 2020 00:00:00 +0000</pubDate>
      
      <guid>/theorypost/r-and-python/</guid>
      <description>body {text-align: justify}.rwd-video {height: 0;overflow: hidden;padding-bottom: 56.25%;padding-top: 30px;position: relative;}.rwd-video iframe,.rwd-video object,.rwd-video embed {height: 100%;left: 0;position: absolute;top: 0;width: 100%;}</description>
    </item>
    
    <item>
      <title>Requirements to Deploy Video Interview</title>
      <link>/theorypost/requirements-to-deply-video-interview/</link>
      <pubDate>Sat, 04 Apr 2020 00:00:00 +0000</pubDate>
      
      <guid>/theorypost/requirements-to-deply-video-interview/</guid>
      <description>body {text-align: justify}.rwd-video {height: 0;overflow: hidden;padding-bottom: 56.25%;padding-top: 30px;position: relative;}.rwd-video iframe,.rwd-video object,.rwd-video embed {height: 100%;left: 0;position: absolute;top: 0;width: 100%;}</description>
    </item>
    
    <item>
      <title>Semantic Analysis</title>
      <link>/theorypost/semantic-analysis/</link>
      <pubDate>Sat, 04 Apr 2020 00:00:00 +0000</pubDate>
      
      <guid>/theorypost/semantic-analysis/</guid>
      <description>body {text-align: justify}.rwd-video {height: 0;overflow: hidden;padding-bottom: 56.25%;padding-top: 30px;position: relative;}.rwd-video iframe,.rwd-video object,.rwd-video embed {height: 100%;left: 0;position: absolute;top: 0;width: 100%;}</description>
    </item>
    
    <item>
      <title>Topic Model Video Interview</title>
      <link>/theorypost/topic-model-video-interview/</link>
      <pubDate>Sat, 04 Apr 2020 00:00:00 +0000</pubDate>
      
      <guid>/theorypost/topic-model-video-interview/</guid>
      <description>body {text-align: justify}.rwd-video {height: 0;overflow: hidden;padding-bottom: 56.25%;padding-top: 30px;position: relative;}.rwd-video iframe,.rwd-video object,.rwd-video embed {height: 100%;left: 0;position: absolute;top: 0;width: 100%;}</description>
    </item>
    
    <item>
      <title>Activation Function in Neural Nets</title>
      <link>/theorypost/activation-function-in-neural-nets/</link>
      <pubDate>Thu, 09 Jan 2020 00:00:00 +0000</pubDate>
      
      <guid>/theorypost/activation-function-in-neural-nets/</guid>
      <description>body {text-align: justify}What an artifical neuron do is to calculate a weighted sum of its input, adds a bias and then decides whether it should be “fired” or not.considering the following function:
\[Y=\sum(\text {weight} * \text {input})+\text {bias}\]
the value of Y can be anything ranging from -inf to +inf. The neuron really doesn’t know the bounds of the value. How do we decide whether the neuron should fire or not?</description>
    </item>
    
    <item>
      <title>Ensemble Learning with Adaptive Boosting</title>
      <link>/theorypost/ensemble-learning-with-adaptive-boosting/</link>
      <pubDate>Fri, 03 Jan 2020 00:00:00 +0000</pubDate>
      
      <guid>/theorypost/ensemble-learning-with-adaptive-boosting/</guid>
      <description>body {text-align: justify}Why Ensemble First of all, ensemble often have lower error than any individual method by themselves. It has also less overfitting leading us to a better performance in the test set. Each kind of learners that we might use has a sort of bias. Combining all of them, can reduce this bias. 
Adaptive Boosting in Random Forest In a Forest of Trees made with ADA Boost the trees are usually just One Node with Two Leaves that are called Stump, so it is a Forest of Stump.</description>
    </item>
    
    <item>
      <title>Ensemble Learning with Gradient Boosting</title>
      <link>/theorypost/ensemble-learning-with-gradient-boosting/</link>
      <pubDate>Fri, 03 Jan 2020 00:00:00 +0000</pubDate>
      
      <guid>/theorypost/ensemble-learning-with-gradient-boosting/</guid>
      <description>body {text-align: justify}Ensemble often have lower error than any individual method by themselves.It has also less overfitting leading us to a better performance in the test set.Each kind of learners that we might use has a sort of bias. Combining all of them, can reduce this bias. 
Gradient Boost vs. AdaBoost We start considering to variable reported int he figure below, and we want to predict weight from these variables using decision tree.</description>
    </item>
    
    <item>
      <title>The Learning Rate</title>
      <link>/theorypost/the-learning-rate/</link>
      <pubDate>Tue, 17 Dec 2019 00:00:00 +0000</pubDate>
      
      <guid>/theorypost/the-learning-rate/</guid>
      <description>body {text-align: justify}Deep learning neural networks are trained using the stochastic gradient descent algorithm.Stochastic gradient descent is an optimization algorithm that estimates the error gradient for the current state of the model using examples from the training dataset, then updates the weights of the model using the backpropagation of errors algorithm.
The amount that the weights are updated during training is referred to as the step size or the learning rate.</description>
    </item>
    
    <item>
      <title>Recommender System of Scientific Paper</title>
      <link>/theorypost/recommender-system-of-scientific-paper/</link>
      <pubDate>Fri, 13 Dec 2019 00:00:00 +0000</pubDate>
      
      <guid>/theorypost/recommender-system-of-scientific-paper/</guid>
      <description>body {text-align: justify}Many scientists consider the search for related work as an extremely time-consuming part of their responsibilities. The enormity of time taken is partly caused bythe increasing number of publications, which grows exponentially at a yearly rate of 3.7%. Instead of entering just keywords, a user may provide entire documents, including reference lists as input and make implicit and explicit ratings to improve recommendations.</description>
    </item>
    
    <item>
      <title>NLP Glossary</title>
      <link>/theorypost/nlp-glossary/</link>
      <pubDate>Wed, 11 Dec 2019 00:00:00 +0000</pubDate>
      
      <guid>/theorypost/nlp-glossary/</guid>
      <description>body {text-align: justify}Text Similarity using Vector Space Model The idea is to find common words within text. The figure below shows an example with a text with common words “”house” and “white”. It represents all texts with common words, calculating the frequency of each word.
For example, looking at the red point (2,7), 2 is the number of times that the word “white” appears in the text and 7 is the number of times that the word “house” appears in the same text.</description>
    </item>
    
    <item>
      <title>Bias Variance Trade-Off</title>
      <link>/theorypost/bias-variance-trade-off/</link>
      <pubDate>Thu, 07 Nov 2019 00:00:00 +0000</pubDate>
      
      <guid>/theorypost/bias-variance-trade-off/</guid>
      <description>body {text-align: justify}The Bias Variance Trade-Off is used to understand the model’s performance and evaluation. We we have a training error that goes down, nut test error starting to go up, the model we created begins to overfit.
Image to have a Linear Regression ML, but is not accurate to replicate the curve of the true relationship between height and weight.The inability for an ML to capture the true relationship is called bias.</description>
    </item>
    
    <item>
      <title>Introduction to Support Vector Machine</title>
      <link>/theorypost/introduction-to-support-vector-machine/</link>
      <pubDate>Wed, 02 Oct 2019 00:00:00 +0000</pubDate>
      
      <guid>/theorypost/introduction-to-support-vector-machine/</guid>
      <description>body {text-align: justify}This article is a summary of the StatQuest video made by Josh Starmer. Click here to see the video explained by Josh Starmer. 
We use as an example the measurement of the Mass of mice (g). The red dots in the figure below represent mice that are not obese and the green dots represent mice obese. Based on this observation, we can pick a threshold, and when we have a new observation that has less mass than the threshold we can classify it as not obese.</description>
    </item>
    
    <item>
      <title>Introduction to Topic Model</title>
      <link>/theorypost/introduction-to-topic-model/</link>
      <pubDate>Thu, 08 Aug 2019 00:00:00 +0000</pubDate>
      
      <guid>/theorypost/introduction-to-topic-model/</guid>
      <description>body {text-align: justify}The Topic Model is a type of statistical model to find the topics that occur in a collection of documents.It is a frequently used text-mining tool for discovery of hidden semantic structures in a text body. For example, image to have some articles or a series of social media messages and we want to understand what is going on inside of them. A common tool to face this problem is via Unsupervised Machine Learning model.</description>
    </item>
    
    <item>
      <title>Introduction to Naive Bayes</title>
      <link>/theorypost/introduction-to-naive-bayes/</link>
      <pubDate>Tue, 06 Aug 2019 00:00:00 +0000</pubDate>
      
      <guid>/theorypost/introduction-to-naive-bayes/</guid>
      <description>body {text-align: justify}It is a Probability Classifier. Naïve Bayes is the first algorithm that should be considered for solving Text Classification Problem which involves High Dimensional training Dataset. A few examples are: Sentiment Analysis and Classifying Topics on Social Media. It also refers to the Bayes’ Theorem also known as Bayes’ Law that give us a method to calculate the Conditional Probability: that is the probability of an event, based on previous knowledge available on the events.</description>
    </item>
    
    <item>
      <title>Introduction to Word2Vec</title>
      <link>/theorypost/introduction-to-word2vec/</link>
      <pubDate>Tue, 06 Aug 2019 00:00:00 +0000</pubDate>
      
      <guid>/theorypost/introduction-to-word2vec/</guid>
      <description>body {text-align: justify}The Word2Vec is a semantic learning framework that used a neural network to learn the representation of words or phrases in a text. It is usefull to understand the semantic meaning behind a term. This algorithm use two methods: 1 - CBOW 2 - SkipGram In Continuous bag of words CBOW predicts the current word from a window of surrounding context words, or given a set of context words predicts the missing word that is likely to appear in that context.</description>
    </item>
    
    <item>
      <title>Introduction to K-Means and Hierarchical clustering</title>
      <link>/theorypost/introduction-to-k-means-and-hierarchical-clustering/</link>
      <pubDate>Fri, 02 Aug 2019 00:00:00 +0000</pubDate>
      
      <guid>/theorypost/introduction-to-k-means-and-hierarchical-clustering/</guid>
      <description>body {text-align: justify}This article is a summary of the StatQuest video made by Josh Starmer. Click here to see the video explained by Josh Starmer. 
K-Means Clustering This is a popular unsupervised machine learning algorithms. The objective of K-means is simple: group similar data points together and discover underlying patterns. The Step - 1 is to identify the number of clusters. Suppose we have K=3, the Step - 2 is to select randomly 3 data points and these are our Initial Cluster Points.</description>
    </item>
    
    <item>
      <title>Ensemble Learning</title>
      <link>/theorypost/ensemble-learning/</link>
      <pubDate>Fri, 26 Jul 2019 00:00:00 +0000</pubDate>
      
      <guid>/theorypost/ensemble-learning/</guid>
      <description>body {text-align: justify}This article is a summary of the StatQuest video made by Josh Starmer. Click here to see the video explained by Josh Starmer. 
The Boosting and Ensemble Learning concepts can be applied to many Machine Learning models: it is a Meta Algorithm used to convert many weak learners into a strong learner in order to achieve good performance in supervised problems.</description>
    </item>
    
    <item>
      <title>Principal Component Analysis</title>
      <link>/theorypost/principal-component-analysis/</link>
      <pubDate>Fri, 26 Jul 2019 00:00:00 +0000</pubDate>
      
      <guid>/theorypost/principal-component-analysis/</guid>
      <description>body {text-align: justify}This article is a summary of the StatQuest video made by Josh Starmer. Click here to see the video explained by Josh Starmer. 
The Principal Component Analysis is a deterministic method (given an input will always produce the same output). It is always good to perform a PCA: Principal Components Analysis (PCA) is a data reduction technique that transforms a larger number of correlated variables into a much smaller set of uncorrelated variables called Principal Components.</description>
    </item>
    
    <item>
      <title>Simple Regression with Python</title>
      <link>/theorypost/simple-regression-with-python/</link>
      <pubDate>Fri, 19 Jul 2019 00:00:00 +0000</pubDate>
      
      <guid>/theorypost/simple-regression-with-python/</guid>
      <description>body { text-align: justify}  The reticulate package provides a comprehensive set of tools fot interoperability between Python and R.
# Data Preprocessing Template # Importing the libraries import numpy as np import matplotlib.pyplot as plt import pandas as pd # print(r.df.head()) # Importing the dataset df = pd.read_csv(r&amp;quot;C:\07 - R Website\dataset\PY\Salary_Data.csv&amp;quot;) # load the data df.head(10) # show firt 10 observations # # Preprocessing Input data  ## YearsExperience Salary ## 0 1.</description>
    </item>
    
    <item>
      <title>Fitting a curve to data LOWESS and LOESS</title>
      <link>/theorypost/fitting-a-curve-to-data-lowess-and-loess/</link>
      <pubDate>Tue, 18 Jun 2019 00:00:00 +0000</pubDate>
      
      <guid>/theorypost/fitting-a-curve-to-data-lowess-and-loess/</guid>
      <description>body {text-align: justify}This article is a summary of the StatQuest video made by Josh Starmer. Click here to see the video explained by Josh Starmer. 
Locally weighted regression scatterplot smoothing - LOWESS The main idea to fit a curve to data point is to use a type of sliding window to divide the data into smaller blobs. The second main idea is that each data point use the least squares to fit a line.</description>
    </item>
    
    <item>
      <title>Ridge and Lasso Regression</title>
      <link>/theorypost/ridge-and-lasso-regression/</link>
      <pubDate>Tue, 18 Jun 2019 00:00:00 +0000</pubDate>
      
      <guid>/theorypost/ridge-and-lasso-regression/</guid>
      <description>body {text-align: justify}This article is a summary of the StatQuest video made by Josh Starmer. Click here to see the video on Ridge Regression explained by Josh Starmer. Click here to see the video on Lasso Regression explained by Josh Starmer. 
Overfitting In statistics, overfitting is the production of an analysis that corresponds too closely or exactly to a particular set of data, and may therefore fail to fit additional data or predict future observations reliably.</description>
    </item>
    
    <item>
      <title>Synthetic Minority Oversampling Technique</title>
      <link>/theorypost/synthetic-minority-oversampling-technique/</link>
      <pubDate>Mon, 17 Jun 2019 00:00:00 +0000</pubDate>
      
      <guid>/theorypost/synthetic-minority-oversampling-technique/</guid>
      <description>body {text-align: justify}Oversampling and undersampling in data analysis are techniques used to adjust the class distribution of a data set. Both oversampling and undersampling involve introducing a bias to select more samples from one class than from another, to compensate for an imbalance that is either already present in the data. Data Imbalance can be of the following types:  Under-representation of a class in one or more important predictor variables.</description>
    </item>
    
    <item>
      <title>Cluster Analysis in Theory</title>
      <link>/theorypost/cluster-analysis-in-theory/</link>
      <pubDate>Wed, 12 Jun 2019 00:00:00 +0000</pubDate>
      
      <guid>/theorypost/cluster-analysis-in-theory/</guid>
      <description>body {text-align: justify}This article is a summary of the StatQuest video made by Josh Starmer. Click here to see the video on K-Mean explained by Josh Starmer. Click here to see the video on Hierarchical Clustering explained by Josh Starmer. 
Cluster analysis or clustering is the task of grouping a set of objects in such a way that objects in the same group (called a cluster) are more similar (in some sense) to each other than to those in other groups (clusters).</description>
    </item>
    
    <item>
      <title>Gradient Descent Step by Step</title>
      <link>/theorypost/gradient-descent-step-by-step/</link>
      <pubDate>Wed, 13 Feb 2019 00:00:00 +0000</pubDate>
      
      <guid>/theorypost/gradient-descent-step-by-step/</guid>
      <description>body {text-align: justify}This article is a summary of the StatQuest video made by Josh Starmer. Click here to see the video explained by Josh Starmer. 
Introduction In statistics, Machine Learning and other Data Science fields, we optimize a lot of stuff. For example in linear regresion, we optimize the Intercept and Slope, or when we use Logistic Regression we optimize the squiggle. Moreover, in t-SNE we optimize clusters.</description>
    </item>
    
    <item>
      <title>Data Science using Agile Methodology</title>
      <link>/theorypost/data-science-using-agile-methodology/</link>
      <pubDate>Mon, 11 Feb 2019 00:00:00 +0000</pubDate>
      
      <guid>/theorypost/data-science-using-agile-methodology/</guid>
      <description>body { text-align: justify}  INTRODUCTION  A data science team asks great questions, explores the data, and delivers key insights.  The best way to generate business value is to deliver a constant stream of key insights in short two-week sprints.  A short sprint will also help the team pivot so they can ask new questions based on what they learn from the data. 
WORK ON A DATA SCIENCE PROJECT  Typical project upfront requirements and we need to understand what we are going to build before to start the planning project.</description>
    </item>
    
    <item>
      <title>Event Processing</title>
      <link>/theorypost/event-processing/</link>
      <pubDate>Wed, 23 Jan 2019 00:00:00 +0000</pubDate>
      
      <guid>/theorypost/event-processing/</guid>
      <description>body {text-align: justify}Process Data Event data consists of three basic components: the why, the what and the who. Analysing event data is an iteractive process of three steps: extraction (from raw data to event log), processing (removing redundant details, enrich data by calculating variables) and analysis. The analysis could be for instance which are the roles of different doctors and nurses organization and how they work together.</description>
    </item>
    
    <item>
      <title>Continuous Probability</title>
      <link>/theorypost/continuous-probability/</link>
      <pubDate>Tue, 22 Jan 2019 00:00:00 +0000</pubDate>
      
      <guid>/theorypost/continuous-probability/</guid>
      <description>body {text-align: justify}Empirical Cumulative Distribution Function  When summarizing a list of numeric values such as heights, it’s not useful to construct a distribution that assigns a proportion to each possible outcome. It is much more practical to define a function that operates on intervals rather than single values. The standard way of doing this is using the cumulative distribution function (CDF).  As an example, we define the empirical cumulative distribution function (eCDF) for heights for male adult students.</description>
    </item>
    
    <item>
      <title>How many Monte Carlo are enough?</title>
      <link>/theorypost/how-many-monte-carlo-are-enough/</link>
      <pubDate>Sun, 20 Jan 2019 00:00:00 +0000</pubDate>
      
      <guid>/theorypost/how-many-monte-carlo-are-enough/</guid>
      <description>body {text-align: justify}Here an example of the The birthday problem solution via Monte Carlo. Suppose you’re in a classroom with 22 people. If we assume this is a randomly selected group, what is the chance that at least two people have the same birthday? 
This is a problem of discrete probability. 
All right, first, note that birthdays can be represented as numbers between 1 and 365.</description>
    </item>
    
    <item>
      <title>The Monty Hall Problem</title>
      <link>/theorypost/the-monty-hall-problem/</link>
      <pubDate>Sun, 20 Jan 2019 00:00:00 +0000</pubDate>
      
      <guid>/theorypost/the-monty-hall-problem/</guid>
      <description>body {text-align: justify}In the 1970s, there was a game show called Let’s Make a Deal. Monty Hall was the hos, this is where the name of the problem comes from. At some point in the game, contestants were asked to pick one of three doors. Behind one door, there was a prize. The other two had a goat behind them. And this basically meant that you lost.</description>
    </item>
    
  </channel>
</rss>