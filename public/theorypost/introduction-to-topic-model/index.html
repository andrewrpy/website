<!DOCTYPE html>
<html lang="en-us">
  <head>
    <meta charset="utf-8">
<meta name="viewport" content="width=device-width, initial-scale=1.0">
<meta name="generator" content="Hugo 0.55.6" />


<title>Introduction to Topic Model - Andrea Perlato</title>
<meta property="og:title" content="Introduction to Topic Model - Andrea Perlato">



  







<link rel="stylesheet" href="/css/fonts.css" media="all">
<link rel="stylesheet" href="/css/main.css" media="all">



  </head>
  <body>
    <div class="wrapper">
      <header class="header">
        <nav class="nav">
  <a href="/" class="nav-logo">
    <img src="/images/logo.png"
         width="50"
         height="50"
         alt="Logo">
  </a>

  <ul class="nav-links">
    
    <li><a href="/about/">About</a></li>
    
    <li><a href="/graphpost/">Graph</a></li>
    
    <li><a href="/mlpost/">Machine Learning</a></li>
    
    <li><a href="/aipost/">Artificial Intelligence</a></li>
    
    <li><a href="/tspost/">Time Series</a></li>
    
    <li><a href="/theorypost/">Theory</a></li>
    
  </ul>
</nav>

      </header>


<main class="content" role="main">

  <article class="article">
    

    <h1 class="article-title">Introduction to Topic Model</h1>

    

    <div class="article-content">
      


<style>
body {
text-align: justify}
</style>
<p>The <a href="https://en.wikipedia.org/wiki/Topic_model"><strong>Topic Model</strong></a> is a type of statistical model to find the topics that occur in a collection of documents.
It is a frequently used text-mining tool for discovery of hidden semantic structures in a text body. For example, image to have some articles or a series of social media messages and we want to understand what is going on inside of them. A common tool to face this problem is via <strong>Unsupervised Machine Learning model</strong>.
From a high level perspective, having a bunch of documents, and like in <a href="https://www.andreaperlato.com/theorypost/introduction-to-k-means-and-hierarchical-clustering/"><strong>K-Mean Clustering</strong></a> we want to find the <strong>K</strong> number of topics that best describe our corpus of text.</p>
<center>
<img src="/img/topics.png" style="width:40.0%" />
</center>
<p>From the figure above, we have three different topics: technology (yellow), business(orange), and arts(blue).
In addition to those topics, we also have an association to the documents to topics. In fact, a document can be entirely a technology topic (i.e. red light, green light, 2-tone led, simplify screen), but also a document that is a sort of mixture of two or more topic (see the grey text in the figure below).</p>
<center>
<img src="/img/mixturetopics.png" style="width:50.0%" />
</center>
<p>Topic Modelcan be seen as a <strong>Matrix Factorization Problem</strong>, where <strong>K</strong> is the number of topics, <strong>M</strong> is the number of documents, and <strong>V</strong> is the size of the vocabulary.</p>
<center>
<img src="/img/matrixfactorization.png" style="width:50.0%" />
</center>
<p>The <strong>MxV</strong> matrix corresponds to the distribution of the words for each of the topics,and to find this the <a href="https://en.wikipedia.org/wiki/Latent_semantic_analysis"><strong>Latent Semantic Analysis</strong></a> is widely used.</p>
<p>An alternative of the <strong>Matrix Factorization Problem</strong> is the <a href="https://en.wikipedia.org/wiki/Generative_model"><strong>Generative Model</strong></a>. </br>
More particularly, the <a href="https://en.wikipedia.org/wiki/Latent_Dirichlet_allocation"><strong>Latent Dirichlet Allocation</strong></a> is commonly used in topic modelling.</p>
<p><span class="math display">\[
P(\boldsymbol{p} | \alpha \boldsymbol{m})=\frac{\Gamma\left(\sum_{k} \alpha m_{k}\right)}{\prod_{k} \Gamma\left(\alpha m_{k}\right)} \prod_{k} p_{k}^{\alpha m_{k}-1}
\]</span></p>
<p>Here above, we have the Dirichlet Distribution Equation, where <strong>alpha</strong> is the variance, and <strong>m</strong> is the mean.</p>
<center>
<img src="/img/uniformlda.png" style="width:40.0%" />
</center>
<p>As described in the figure above, when we have a <strong>uniform mean</strong> (1/3), and an <strong>variance</strong> (alpha) of three, when we molltiply them togheter, the <strong>LDA=1</strong>, and so each topic is equally likely.
But when the <strong>variance</strong> is larger and larged the LDA is concentrating the distribution around the mean (the dark circle in the middle of the triangle). </br></p>
<p>There are other ways to parametrize the LDA distribution. </br>
For example, we can have mean in a differnt location (see the left and center figure below). </br>
We can also have the <strong>variance parameter alpha</strong> smaller than <strong>1</strong>. In this case we push the probability mass to the edges of the simplex (see the right figure below). In this case, with <strong>alpha&lt;1</strong> we have a preference for the multinomial distribution which is far avay for the center.
To be <strong>far away from the center</strong> means we have not a precise topic to assign to our word. It is similar to how people write document where many things are inside a concept.
In other words, the Dirichlet Distribution give us a distribution over all the places where the Multinomila Distribution can land.</p>
<center>
<img src="/img/nonuniformlda.png" style="width:40.0%" />
</center>
<p>The <strong>Dirichlet Distribution</strong> can be used to isolate which document is about a specific topic.
For each topic <strong>K</strong>, we have a multinomial distribution <strong>Betak</strong>, called <strong>Topic Distribution</strong> from a Dirichlet Distribution with parameter <strong>lambda</strong>.
The next step is to draw a <strong>multinomial distribution over topics</strong> represented by <strong>Өd</strong>. Once we have it, we can draw for each word the so called <strong>Topic Assignment</strong> represented in the figure below by <strong>Zn</strong>.
Till now, we don’t know what the word is. We have to look at the <strong>Topic Distribution Betak</strong> in order to generate the word that comes from the multinomial distribution.</p>
<center>
<img src="/img/ldaplatenotation.png" style="width:40.0%" />
</center>
<p>The graph above is a representation of the probabilistic model, also called <a href="https://en.wikipedia.org/wiki/Plate_notation"><strong>Plate Notation</strong></a>. </br>
As we can see we have in the Plate Notation <strong>K</strong> topics in <strong>M</strong> documents, and <strong>N</strong> words in each document. </br>
Crucially, the only thing that we observe are the <strong>words</strong>, and our task is to figure out what topic to assign <strong>Zn</strong>.</p>
<p>Ideally, once we have the collection of words per topic, is the topic is interpretable, people will consistently choose true <strong>Intruder</strong>, or define the words that didn’t belong. </br>
To learn more about LDA please check out this <a href="http://www.jmlr.org/papers/volume3/blei03a/blei03a.pdf"><strong>link</strong></a>.</p>

    </div>
  </article>

  


</main>

      <footer class="footer">
        <ul class="footer-links">
          <li>
            <a href="/index.xml" type="application/rss+xml" target="_blank">RSS feed</a>
          </li>
          <li>
            <a href="https://www.rstudio.com/" class="footer-links-kudos">Made with <img src="/images/hugo-logo.png" alt="Img link to Hugo website" width="22" height="22"></a>
          </li>
        </ul>
      </footer>

    </div>
    

    
<script src="/js/math-code.js"></script>
<script async src="//cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.5/MathJax.js?config=TeX-MML-AM_CHTML"></script>


    
<script type="application/javascript">
var doNotTrack = false;
if (!doNotTrack) {
	(function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
	(i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
	m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
	})(window,document,'script','https://www.google-analytics.com/analytics.js','ga');
	ga('create', 'UA-51254710-89', 'auto');
	
	ga('send', 'pageview');
}
</script>

  </body>
</html>

